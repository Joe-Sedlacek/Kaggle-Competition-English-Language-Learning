{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "import numpy\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "import unicodecsv\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,gender,age,topic,sign,date,text\\n2059027,male,15,Student,Leo,\"14,May,2004\",\"           Info has been found (+/- 100 pages, and 4.5 MB of .pdf files) Now i have to wait untill our team leader has processed it and learns html.         \"\\n2059027,male,15,Student,Leo,\"13,May,2004\",\"           These are the team members:   Drewes van der Laag           urlLink mail  Ruiyu Xie                     urlLink mail  Bryan Aaldering (me)          urlLink mail          \"\\n2059027,male,15,Student,Leo,\"12,May,2004\",\"           In het kader van kernfusie op aarde:  MAAK JE EIGEN WATERSTOFBOM   How to build an H-Bomb From: ascott@tartarus.uwa.edu.au (Andrew Scott) Newsgroups: rec.humor Subject: How To Build An H-Bomb (humorous!) Date: 7 Feb 1994 07:41:14 GMT Organization: The University of Western Australia  Original file dated 12th November 1990. Seemed to be a transcript of a \\'Seven Days\\' article. Poorly formatted and corrupted. I have added the text between \\'examine under a microscope\\' and \\'malleable, like gold,\\' as it was missing. If anyone has the full text, please distribute. I am not responsible for the accuracy of this information. Converted to HTML by Dionisio@InfiNet.com 11/13/98. (Did a little spell-checking and some minor edits too.) Stolen from  urlLink http://my.ohio.voyager.net/~dionisio/fun/m...own-h-bomb.html  and reformatted the HTML. It now validates to XHTML 1.0 Strict. How to Build an H-Bomb Making and owning an H-bomb is the kind of challenge real Americans seek. Who wants to be a passive victim of nuclear war when, with a little effort, you can be an active participant? Bomb shelters are for losers. Who wants to huddle together underground eating canned Spam? Winners want to push the button themselves. Making your own H-bomb is a big step in nuclear assertiveness training -- it\\'s called Taking Charge. We\\'re sure you\\'ll enjoy the risks and the heady thrill of playing nuclear chicken. Introduction When the Feds clamped down on The Progressive magazine for attempting to publish an article on the manufacture of the hydrogen bomb, it piqued our curiosity. Was it really true that atomic and hydrogen bomb technology was so simple you could build an H-bomb in your own kitchen? Seven Days decided to find out. Food editor Barbara Ehrenreich, investigative reporter Peter Biskind, Photographer Jane Melnick and nuclear scientist Michio Kaku were given three days to cook up a workable H-bomb. They did and we have decided to share their culinary secrets with you. Not that Seven Days supports nuclear terrorism. We don\\'t. We would prefer to die slowly from familiar poisons like low-level radiation, microwaves, DDT, DBCP, aflatoxins, PBBs, PBCs, or food dyes, rather than unexpectedly, say as hostage to a Latvian nationalist brandishing a homemade bomb. In our view the real terrorists are the governments, American, Soviet, French, Chinese, and British, that are hoarding H-bombs for their own use, and worse still, those governments (U.S., French and German) that are eagerly peddling advanced nuclear technology to countries like South Africa, Brazil, and Argentina so that they can make their own bombs. When these bombs are used, and they will be, it will be the world\\'s big-time nuclear peddlers, along with corporate suppliers like General Electric, Westinghouse, and Gulf Oil, that we can thank for it. Gagging The Progressive will do no more for national security than backyard bomb shelters because like it or not the news is out. The heart of the successful H-bomb is the successful A-bomb. Once you\\'ve got your A-bombs made the rest is frosting on the cake. All you have to do is set them up so that when they detonate they\\'ll start off a hydrogen-fusion reaction.  Part 1: Making Your Bomb Step 1: Getting the Ingredients Uranium is the basic ingredient of the A-bomb. When a uranium atom\\'s nucleus splits apart, it releases a tremendous amount of energy (for its size), and it emits neutrons which go on to split other nearby uranium nuclei, releasing more energy, in what is called a \\'chain reaction\\'. (When atoms split, matter is converted into energy according to Einstein\\'s equation E=MC2. What better way to mark his birthday than with your own atomic fireworks?) There are two kinds (isotopes) of uranium: the rare U-235, used in bombs, and the more common, heavier, but useless U-238. Natural uranium contains less than 1 percent U-235 and in order to be usable in bombs it has to be \\'enriched\\' to 90 percent U-235 and only 10 percent U-238. Plutonium-239 can also be used in bombs as a substitute for U-235. Ten pounds of U-235 (or slightly less plutonium) is all that is necessary for a bomb. Less than ten pounds won\\'t give you a critical mass. So purifying or enriching naturally occurring uranium is likely to be your first big hurdle. It is infinitely easy to steal ready-to-use enriched uranium or plutonium than to enrich some yourself. And stealing uranium is not as hard as it sounds. There are at least three sources of enriched uranium or plutonium... Enriched uranium is manufactured at a gaseous diffusion plant in Portsmouth, Ohio. From there it is shipped in 10 liter bottles by airplane and trucks to conversion plants that turn it into uranium oxide or uranium metal. Each 10 liter bottle contains 7 kilograms of U-235, and there are 20 bottles to a typical shipment. Conversion facilities exist at Hematite, Missouri; Apollo, Pennsylvania; and Erwin, Tennessee. The Kerr-McGee plant at Crescent Oklahoma -- where Karen Silkwood worked -- was a conversion plant that \\'lost\\' 40 lbs of plutonium. Enriched uranium can be stolen from these plants or from fuel-fabricating plants like those in New Haven, San Diego; or Lynchburg, Virginia. (A former Kerr-McGee supervisor, James V. Smith, when asked at the Silkwood trial if there were any security precautions at the plant to prevent theft, testified that \\'There were none of any kind, no guards, no fences, no nothing.\\') Plutonium can be obtained from places like United Nuclear in Pawling, New York; Nuclear Fuel Services in Erwin, Tennessee; General Electric in Pleasanton, California; Westinghouse in Cheswick, Pennsylvania; Nuclear Materials and Equipment Corporation (NUMEC) in Leechburg, Pennsylvania; and plants in Hanfford, Washington and Morris, Illinois. According to Rolling Stone magazine the Israelis were involved in the theft of plutonium from NUMEC. Finally you can steal enriched uranium or plutonium while it\\'s en-route from conversion plants to fuel fabricating plants. It is usually transported (by air or truck) in the form of uranium oxide, a brownish powder resembling instant coffee, or as a metal, coming in small chunks called \\'broken buttons.\\' Both forms are shipped in small cans stacked in 5-inch cylinders braced with welded struts in the center of ordinary 55 gallon steel drums. The drums weigh about 100 pounds and are clearly marked \\'Fissible Material\\' or \\'Danger, Plutonium.\\' A typical shipment might go from the enrichment plant at Portsmouth, Ohio to the conversion plant in Hematite Missouri then to Kansas City by truck where it would be flown to Los Angeles and then trucked down to the General Atomic plant in San Diego. The plans for the General Atomic plant are on file at the Nuclear Regulatory Commission\\'s reading room at 1717 H Street NW Washington. A Xerox machine is provided for the convenience of the public. If you can\\'t get hold of any enriched uranium you\\'ll have to settle for commercial grade (20 percent U-235). This can be stolen from university reactors of a type called TRIGA Mark II, where security is even more casual than at commercial plants. If stealing uranium seems too tacky you can buy it. Unenriched uranium is available at any chemical supply house for $23 a pound. Commercial grade (3 to 20 percent enriched) is available for $40 a pound from Gulf Atomic. You\\'ll have to enrich it further yourself. Quite frankly this can be something of a pain in the ass. You\\'ll need to start with a little more than 50 pounds of commercial-grade uranium. (It\\'s only 20 percent U-235 at best, and you need 10 pounds of U-235 so... ) But with a little kitchen-table chemistry you\\'ll be able to convert the solid uranium oxide you\\'ve purchased into a liquid form. Once you\\'ve done that, you\\'ll be able to separate the U-235 that you\\'ll need from the U-238. First pour a few gallons of concentrated hydrofluoric acid into your uranium oxide, converting it to uranium tetrafluoride. (Safety note: Concentrated hydrofluoric acid is so corrosive that it will eat its way through glass, so store it only in plastic. Used 1-gallon plastic milk containers will do.) Now you have to convert your uranium tetrafluoride to uranium hexafluoride, the gaseous form of uranium, which is convenient for separating out the isotope U-235 from U-238. To get the hexafluoride form, bubble fluorine gas into your container of uranium tetrafluoride. Fluorine is available in pressurized tanks from chemical-supply firms. Be careful how you use it though because fluorine is several times more deadly than chlorine, the classic World War I poison gas. Chemists recommend that you carry out this step under a stove hood (the kind used to remove unpleasant cooking odors). If you\\'ve done your chemistry right you should now have a generous supply of uranium hexafluoride ready for enriching. In the old horse-and-buggy days of A-bomb manufacture the enrichment was carried out by passing the uranium hexafluoride through hundreds of miles of pipes, tubes, and membranes, until the U-235 was eventually separated from the U-238. This gaseous-diffusion process, as it was called is difficult, time-consuming, and expensive. Gaseous-diffusion plants cover hundreds of acres and cost in the neighborhood of $2-billion each. So forget it. There are easier, and cheaper, ways to enrich your uranium. First transform the gas into a liquid by subjecting it to pressure. You can use a bicycle pump for this. Then make a simple home centrifuge. Fill a stan'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading processed data\n",
    "data = open(r'C:\\Users\\jsedl\\Downloads\\blogtext.csv', encoding='utf-8').read()[:10000]\n",
    "data\n",
    "#data=unicodecsv.reader(data, encoding='utf-8', delimeter = ';')\n",
    "#data = \"I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home. The hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear. most students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go. when your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class. \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to read with pandas...may need if cannot fix w other\n",
    "# myFile=open(r'C:\\Users\\jsedl\\Downloads\\blogtext.csv')\n",
    "# data=pd.read_csv(myFile, encoding = 'utf-8', quotechar='\"' ,delimiter = ';', nrows=1000000)\n",
    "# type(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for preparing text data into sequences for training \n",
    "def data_sequencing(data):   \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([data])\n",
    "    with open('tokenizer.pkl', 'wb') as f: # Save the tokeniser by pickling it\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "    # retrieve vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocabulary Size: %d' % vocab_size)\n",
    "    \n",
    "    # create line-based sequences\n",
    "    sequences = list()\n",
    "    rev_sequences = list()\n",
    "    for line in data.split('.'):\n",
    "        encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "        rev_encoded = encoded[::-1]\n",
    "        for i in range(1, len(encoded)):\n",
    "            sequence = encoded[:i+1]\n",
    "            rev_sequence = rev_encoded[:i+1]\n",
    "            sequences.append(sequence)\n",
    "            rev_sequences.append(rev_sequence)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    \n",
    "    #find max sequence length \n",
    "    max_length = max([len(seq) for seq in sequences])\n",
    "    with open('max_length.pkl', 'wb') as f: # Save max_length by pickling it\n",
    "        pickle.dump(max_length, f)\n",
    "    print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "    # pad sequences and create the forward sequence\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    sequences = array(sequences)\n",
    "    X, y = sequences[:,:-1],sequences[:,-1]\n",
    "    \n",
    "    #pad sequences and create the reverse sequencing\n",
    "    rev_sequences = pad_sequences(rev_sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    rev_sequences = array(rev_sequences)\n",
    "    rev_X, rev_y = rev_sequences[:,:-1],rev_sequences[:,-1]\n",
    "\n",
    "    return X,y,rev_X,rev_y,max_length,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 740\n",
      "Total Sequences: 1599\n",
      "Max Sequence Length: 56\n"
     ]
    }
   ],
   "source": [
    "#returning forward and reverse sequences along with max sequence \n",
    "#length from the data \n",
    "\n",
    "X,y,rev_X,rev_y,max_length,vocab_size = data_sequencing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 55, 100)           74000     \n",
      "                                                                 \n",
      " bidirectional_12 (Bidirecti  (None, 200)              160800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 740)               148740    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 383,540\n",
      "Trainable params: 383,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define forward sequence model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,100, input_length=max_length-1))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 55, 100)           74000     \n",
      "                                                                 \n",
      " bidirectional_13 (Bidirecti  (None, 200)              160800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 740)               148740    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 383,540\n",
      "Trainable params: 383,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define reverse model\n",
    "rev_model = Sequential()\n",
    "rev_model.add(Embedding(vocab_size, 100, input_length=max_length-1))\n",
    "#rev_model.add(LSTM(100))\n",
    "rev_model.add(Bidirectional(LSTM(100)))\n",
    "rev_model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(rev_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 15s - loss: 6.5485 - accuracy: 0.0225 - 15s/epoch - 967ms/step\n",
      "Epoch 2/100\n",
      "16/16 - 13s - loss: 6.0941 - accuracy: 0.0275 - 13s/epoch - 782ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 13s - loss: 5.9467 - accuracy: 0.0356 - 13s/epoch - 814ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 25s - loss: 5.9154 - accuracy: 0.0356 - 25s/epoch - 2s/step\n",
      "Epoch 5/100\n",
      "16/16 - 26s - loss: 5.9097 - accuracy: 0.0281 - 26s/epoch - 2s/step\n",
      "Epoch 6/100\n",
      "16/16 - 28s - loss: 5.8967 - accuracy: 0.0356 - 28s/epoch - 2s/step\n",
      "Epoch 7/100\n",
      "16/16 - 31s - loss: 5.8852 - accuracy: 0.0356 - 31s/epoch - 2s/step\n",
      "Epoch 8/100\n",
      "16/16 - 30s - loss: 5.8683 - accuracy: 0.0394 - 30s/epoch - 2s/step\n",
      "Epoch 9/100\n",
      "16/16 - 30s - loss: 5.8389 - accuracy: 0.0356 - 30s/epoch - 2s/step\n",
      "Epoch 10/100\n",
      "16/16 - 30s - loss: 5.7867 - accuracy: 0.0482 - 30s/epoch - 2s/step\n",
      "Epoch 11/100\n",
      "16/16 - 30s - loss: 5.7152 - accuracy: 0.0500 - 30s/epoch - 2s/step\n",
      "Epoch 12/100\n",
      "16/16 - 30s - loss: 5.6128 - accuracy: 0.0557 - 30s/epoch - 2s/step\n",
      "Epoch 13/100\n",
      "16/16 - 30s - loss: 5.4934 - accuracy: 0.0682 - 30s/epoch - 2s/step\n",
      "Epoch 14/100\n",
      "16/16 - 30s - loss: 5.3746 - accuracy: 0.0763 - 30s/epoch - 2s/step\n",
      "Epoch 15/100\n",
      "16/16 - 30s - loss: 5.2399 - accuracy: 0.0788 - 30s/epoch - 2s/step\n",
      "Epoch 16/100\n",
      "16/16 - 30s - loss: 5.1088 - accuracy: 0.1069 - 30s/epoch - 2s/step\n",
      "Epoch 17/100\n",
      "16/16 - 29s - loss: 4.9738 - accuracy: 0.1132 - 29s/epoch - 2s/step\n",
      "Epoch 18/100\n",
      "16/16 - 30s - loss: 4.8435 - accuracy: 0.1182 - 30s/epoch - 2s/step\n",
      "Epoch 19/100\n",
      "16/16 - 29s - loss: 4.7113 - accuracy: 0.1326 - 29s/epoch - 2s/step\n",
      "Epoch 20/100\n",
      "16/16 - 30s - loss: 4.5846 - accuracy: 0.1401 - 30s/epoch - 2s/step\n",
      "Epoch 21/100\n",
      "16/16 - 30s - loss: 4.4541 - accuracy: 0.1639 - 30s/epoch - 2s/step\n",
      "Epoch 22/100\n",
      "16/16 - 29s - loss: 4.3238 - accuracy: 0.1682 - 29s/epoch - 2s/step\n",
      "Epoch 23/100\n",
      "16/16 - 30s - loss: 4.1990 - accuracy: 0.1782 - 30s/epoch - 2s/step\n",
      "Epoch 24/100\n",
      "16/16 - 30s - loss: 4.0763 - accuracy: 0.1951 - 30s/epoch - 2s/step\n",
      "Epoch 25/100\n",
      "16/16 - 29s - loss: 3.9545 - accuracy: 0.2095 - 29s/epoch - 2s/step\n",
      "Epoch 26/100\n",
      "16/16 - 19s - loss: 3.8294 - accuracy: 0.2233 - 19s/epoch - 1s/step\n",
      "Epoch 27/100\n",
      "16/16 - 13s - loss: 3.7038 - accuracy: 0.2414 - 13s/epoch - 839ms/step\n",
      "Epoch 28/100\n",
      "16/16 - 13s - loss: 3.5935 - accuracy: 0.2614 - 13s/epoch - 833ms/step\n",
      "Epoch 29/100\n",
      "16/16 - 13s - loss: 3.4805 - accuracy: 0.2864 - 13s/epoch - 836ms/step\n",
      "Epoch 30/100\n",
      "16/16 - 13s - loss: 3.3680 - accuracy: 0.3102 - 13s/epoch - 843ms/step\n",
      "Epoch 31/100\n",
      "16/16 - 14s - loss: 3.2538 - accuracy: 0.3321 - 14s/epoch - 860ms/step\n",
      "Epoch 32/100\n",
      "16/16 - 14s - loss: 3.1464 - accuracy: 0.3515 - 14s/epoch - 863ms/step\n",
      "Epoch 33/100\n",
      "16/16 - 23s - loss: 3.0515 - accuracy: 0.3621 - 23s/epoch - 1s/step\n",
      "Epoch 34/100\n",
      "16/16 - 30s - loss: 2.9415 - accuracy: 0.4009 - 30s/epoch - 2s/step\n",
      "Epoch 35/100\n",
      "16/16 - 30s - loss: 2.8401 - accuracy: 0.4203 - 30s/epoch - 2s/step\n",
      "Epoch 36/100\n",
      "16/16 - 30s - loss: 2.7412 - accuracy: 0.4515 - 30s/epoch - 2s/step\n",
      "Epoch 37/100\n",
      "16/16 - 30s - loss: 2.6464 - accuracy: 0.4709 - 30s/epoch - 2s/step\n",
      "Epoch 38/100\n",
      "16/16 - 30s - loss: 2.5511 - accuracy: 0.4991 - 30s/epoch - 2s/step\n",
      "Epoch 39/100\n",
      "16/16 - 30s - loss: 2.4612 - accuracy: 0.5216 - 30s/epoch - 2s/step\n",
      "Epoch 40/100\n",
      "16/16 - 30s - loss: 2.3765 - accuracy: 0.5453 - 30s/epoch - 2s/step\n",
      "Epoch 41/100\n",
      "16/16 - 29s - loss: 2.2892 - accuracy: 0.5729 - 29s/epoch - 2s/step\n",
      "Epoch 42/100\n",
      "16/16 - 28s - loss: 2.1966 - accuracy: 0.6041 - 28s/epoch - 2s/step\n",
      "Epoch 43/100\n",
      "16/16 - 29s - loss: 2.1288 - accuracy: 0.6341 - 29s/epoch - 2s/step\n",
      "Epoch 44/100\n",
      "16/16 - 28s - loss: 2.0427 - accuracy: 0.6423 - 28s/epoch - 2s/step\n",
      "Epoch 45/100\n",
      "16/16 - 28s - loss: 1.9627 - accuracy: 0.6760 - 28s/epoch - 2s/step\n",
      "Epoch 46/100\n",
      "16/16 - 30s - loss: 1.8900 - accuracy: 0.6961 - 30s/epoch - 2s/step\n",
      "Epoch 47/100\n",
      "16/16 - 30s - loss: 1.8191 - accuracy: 0.7192 - 30s/epoch - 2s/step\n",
      "Epoch 48/100\n",
      "16/16 - 29s - loss: 1.7437 - accuracy: 0.7455 - 29s/epoch - 2s/step\n",
      "Epoch 49/100\n",
      "16/16 - 30s - loss: 1.6769 - accuracy: 0.7561 - 30s/epoch - 2s/step\n",
      "Epoch 50/100\n",
      "16/16 - 29s - loss: 1.6118 - accuracy: 0.7761 - 29s/epoch - 2s/step\n",
      "Epoch 51/100\n",
      "16/16 - 27s - loss: 1.5513 - accuracy: 0.7924 - 27s/epoch - 2s/step\n",
      "Epoch 52/100\n",
      "16/16 - 15s - loss: 1.4868 - accuracy: 0.8018 - 15s/epoch - 963ms/step\n",
      "Epoch 53/100\n",
      "16/16 - 15s - loss: 1.4259 - accuracy: 0.8205 - 15s/epoch - 942ms/step\n",
      "Epoch 54/100\n",
      "16/16 - 15s - loss: 1.3686 - accuracy: 0.8286 - 15s/epoch - 950ms/step\n",
      "Epoch 55/100\n",
      "16/16 - 15s - loss: 1.3154 - accuracy: 0.8424 - 15s/epoch - 960ms/step\n",
      "Epoch 56/100\n",
      "16/16 - 15s - loss: 1.2608 - accuracy: 0.8474 - 15s/epoch - 958ms/step\n",
      "Epoch 57/100\n",
      "16/16 - 16s - loss: 1.2085 - accuracy: 0.8630 - 16s/epoch - 974ms/step\n",
      "Epoch 58/100\n",
      "16/16 - 16s - loss: 1.1576 - accuracy: 0.8718 - 16s/epoch - 973ms/step\n",
      "Epoch 59/100\n",
      "16/16 - 16s - loss: 1.1075 - accuracy: 0.8755 - 16s/epoch - 976ms/step\n",
      "Epoch 60/100\n",
      "16/16 - 23s - loss: 1.0572 - accuracy: 0.8856 - 23s/epoch - 1s/step\n",
      "Epoch 61/100\n",
      "16/16 - 19s - loss: 1.0161 - accuracy: 0.8906 - 19s/epoch - 1s/step\n",
      "Epoch 62/100\n",
      "16/16 - 17s - loss: 0.9778 - accuracy: 0.8993 - 17s/epoch - 1s/step\n",
      "Epoch 63/100\n",
      "16/16 - 17s - loss: 0.9396 - accuracy: 0.9068 - 17s/epoch - 1s/step\n",
      "Epoch 64/100\n",
      "16/16 - 17s - loss: 0.8986 - accuracy: 0.9131 - 17s/epoch - 1s/step\n",
      "Epoch 65/100\n",
      "16/16 - 17s - loss: 0.8614 - accuracy: 0.9181 - 17s/epoch - 1s/step\n",
      "Epoch 66/100\n",
      "16/16 - 18s - loss: 0.8225 - accuracy: 0.9250 - 18s/epoch - 1s/step\n",
      "Epoch 67/100\n",
      "16/16 - 17s - loss: 0.7859 - accuracy: 0.9281 - 17s/epoch - 1s/step\n",
      "Epoch 68/100\n",
      "16/16 - 17s - loss: 0.7579 - accuracy: 0.9325 - 17s/epoch - 1s/step\n",
      "Epoch 69/100\n",
      "16/16 - 17s - loss: 0.7302 - accuracy: 0.9393 - 17s/epoch - 1s/step\n",
      "Epoch 70/100\n",
      "16/16 - 25s - loss: 0.7020 - accuracy: 0.9375 - 25s/epoch - 2s/step\n",
      "Epoch 71/100\n",
      "16/16 - 29s - loss: 0.6691 - accuracy: 0.9450 - 29s/epoch - 2s/step\n",
      "Epoch 72/100\n",
      "16/16 - 30s - loss: 0.6462 - accuracy: 0.9481 - 30s/epoch - 2s/step\n",
      "Epoch 73/100\n",
      "16/16 - 30s - loss: 0.6187 - accuracy: 0.9531 - 30s/epoch - 2s/step\n",
      "Epoch 74/100\n",
      "16/16 - 30s - loss: 0.5938 - accuracy: 0.9525 - 30s/epoch - 2s/step\n",
      "Epoch 75/100\n",
      "16/16 - 30s - loss: 0.5717 - accuracy: 0.9537 - 30s/epoch - 2s/step\n",
      "Epoch 76/100\n",
      "16/16 - 30s - loss: 0.5472 - accuracy: 0.9587 - 30s/epoch - 2s/step\n",
      "Epoch 77/100\n",
      "16/16 - 30s - loss: 0.5260 - accuracy: 0.9568 - 30s/epoch - 2s/step\n",
      "Epoch 78/100\n",
      "16/16 - 29s - loss: 0.5038 - accuracy: 0.9619 - 29s/epoch - 2s/step\n",
      "Epoch 79/100\n",
      "16/16 - 30s - loss: 0.4859 - accuracy: 0.9631 - 30s/epoch - 2s/step\n",
      "Epoch 80/100\n",
      "16/16 - 30s - loss: 0.4686 - accuracy: 0.9644 - 30s/epoch - 2s/step\n",
      "Epoch 81/100\n",
      "16/16 - 30s - loss: 0.4577 - accuracy: 0.9606 - 30s/epoch - 2s/step\n",
      "Epoch 82/100\n",
      "16/16 - 30s - loss: 0.4461 - accuracy: 0.9662 - 30s/epoch - 2s/step\n",
      "Epoch 83/100\n",
      "16/16 - 31s - loss: 0.4253 - accuracy: 0.9662 - 31s/epoch - 2s/step\n",
      "Epoch 84/100\n",
      "16/16 - 32s - loss: 0.4104 - accuracy: 0.9681 - 32s/epoch - 2s/step\n",
      "Epoch 85/100\n",
      "16/16 - 32s - loss: 0.3955 - accuracy: 0.9669 - 32s/epoch - 2s/step\n",
      "Epoch 86/100\n",
      "16/16 - 32s - loss: 0.3789 - accuracy: 0.9675 - 32s/epoch - 2s/step\n",
      "Epoch 87/100\n",
      "16/16 - 32s - loss: 0.3760 - accuracy: 0.9662 - 32s/epoch - 2s/step\n",
      "Epoch 88/100\n",
      "16/16 - 31s - loss: 0.3803 - accuracy: 0.9644 - 31s/epoch - 2s/step\n",
      "Epoch 89/100\n",
      "16/16 - 31s - loss: 0.3630 - accuracy: 0.9675 - 31s/epoch - 2s/step\n",
      "Epoch 90/100\n",
      "16/16 - 32s - loss: 0.3525 - accuracy: 0.9650 - 32s/epoch - 2s/step\n",
      "Epoch 91/100\n",
      "16/16 - 31s - loss: 0.3345 - accuracy: 0.9731 - 31s/epoch - 2s/step\n",
      "Epoch 92/100\n",
      "16/16 - 31s - loss: 0.3129 - accuracy: 0.9762 - 31s/epoch - 2s/step\n",
      "Epoch 93/100\n",
      "16/16 - 27s - loss: 0.2983 - accuracy: 0.9781 - 27s/epoch - 2s/step\n",
      "Epoch 94/100\n",
      "16/16 - 14s - loss: 0.2846 - accuracy: 0.9781 - 14s/epoch - 882ms/step\n",
      "Epoch 95/100\n",
      "16/16 - 14s - loss: 0.2754 - accuracy: 0.9787 - 14s/epoch - 882ms/step\n",
      "Epoch 96/100\n",
      "16/16 - 14s - loss: 0.2655 - accuracy: 0.9762 - 14s/epoch - 895ms/step\n",
      "Epoch 97/100\n",
      "16/16 - 14s - loss: 0.2561 - accuracy: 0.9794 - 14s/epoch - 879ms/step\n",
      "Epoch 98/100\n",
      "16/16 - 14s - loss: 0.2471 - accuracy: 0.9794 - 14s/epoch - 886ms/step\n",
      "Epoch 99/100\n",
      "16/16 - 14s - loss: 0.2408 - accuracy: 0.9800 - 14s/epoch - 867ms/step\n",
      "Epoch 100/100\n",
      "16/16 - 14s - loss: 0.2314 - accuracy: 0.9806 - 14s/epoch - 903ms/step\n"
     ]
    }
   ],
   "source": [
    "# compile forward sequence network\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y,batch_size=100, epochs=100, verbose=2)\n",
    "# save the model to file\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 11s - loss: 6.5387 - accuracy: 0.0275 - 11s/epoch - 680ms/step\n",
      "Epoch 2/100\n",
      "16/16 - 8s - loss: 6.0607 - accuracy: 0.0344 - 8s/epoch - 488ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 8s - loss: 5.9114 - accuracy: 0.0344 - 8s/epoch - 493ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 8s - loss: 5.8846 - accuracy: 0.0381 - 8s/epoch - 493ms/step\n",
      "Epoch 5/100\n",
      "16/16 - 8s - loss: 5.8680 - accuracy: 0.0375 - 8s/epoch - 490ms/step\n",
      "Epoch 6/100\n",
      "16/16 - 8s - loss: 5.8488 - accuracy: 0.0400 - 8s/epoch - 489ms/step\n",
      "Epoch 7/100\n",
      "16/16 - 8s - loss: 5.8125 - accuracy: 0.0388 - 8s/epoch - 495ms/step\n",
      "Epoch 8/100\n",
      "16/16 - 8s - loss: 5.7752 - accuracy: 0.0500 - 8s/epoch - 492ms/step\n",
      "Epoch 9/100\n",
      "16/16 - 8s - loss: 5.7206 - accuracy: 0.0438 - 8s/epoch - 492ms/step\n",
      "Epoch 10/100\n",
      "16/16 - 8s - loss: 5.6321 - accuracy: 0.0475 - 8s/epoch - 492ms/step\n",
      "Epoch 11/100\n",
      "16/16 - 8s - loss: 5.5474 - accuracy: 0.0519 - 8s/epoch - 495ms/step\n",
      "Epoch 12/100\n",
      "16/16 - 8s - loss: 5.4513 - accuracy: 0.0550 - 8s/epoch - 496ms/step\n",
      "Epoch 13/100\n",
      "16/16 - 8s - loss: 5.3432 - accuracy: 0.0613 - 8s/epoch - 494ms/step\n",
      "Epoch 14/100\n",
      "16/16 - 8s - loss: 5.2720 - accuracy: 0.0625 - 8s/epoch - 497ms/step\n",
      "Epoch 15/100\n",
      "16/16 - 8s - loss: 5.1555 - accuracy: 0.0819 - 8s/epoch - 494ms/step\n",
      "Epoch 16/100\n",
      "16/16 - 8s - loss: 5.0308 - accuracy: 0.0819 - 8s/epoch - 504ms/step\n",
      "Epoch 17/100\n",
      "16/16 - 8s - loss: 4.9267 - accuracy: 0.0988 - 8s/epoch - 500ms/step\n",
      "Epoch 18/100\n",
      "16/16 - 8s - loss: 4.7958 - accuracy: 0.1113 - 8s/epoch - 500ms/step\n",
      "Epoch 19/100\n",
      "16/16 - 8s - loss: 4.6867 - accuracy: 0.1276 - 8s/epoch - 522ms/step\n",
      "Epoch 20/100\n",
      "16/16 - 8s - loss: 4.5781 - accuracy: 0.1388 - 8s/epoch - 527ms/step\n",
      "Epoch 21/100\n",
      "16/16 - 8s - loss: 4.4530 - accuracy: 0.1432 - 8s/epoch - 505ms/step\n",
      "Epoch 22/100\n",
      "16/16 - 8s - loss: 4.3416 - accuracy: 0.1551 - 8s/epoch - 505ms/step\n",
      "Epoch 23/100\n",
      "16/16 - 8s - loss: 4.2411 - accuracy: 0.1676 - 8s/epoch - 508ms/step\n",
      "Epoch 24/100\n",
      "16/16 - 8s - loss: 4.1335 - accuracy: 0.1801 - 8s/epoch - 500ms/step\n",
      "Epoch 25/100\n",
      "16/16 - 8s - loss: 4.0331 - accuracy: 0.1895 - 8s/epoch - 501ms/step\n",
      "Epoch 26/100\n",
      "16/16 - 8s - loss: 3.9303 - accuracy: 0.2033 - 8s/epoch - 499ms/step\n",
      "Epoch 27/100\n",
      "16/16 - 8s - loss: 3.8331 - accuracy: 0.2176 - 8s/epoch - 502ms/step\n",
      "Epoch 28/100\n",
      "16/16 - 8s - loss: 3.7405 - accuracy: 0.2364 - 8s/epoch - 502ms/step\n",
      "Epoch 29/100\n",
      "16/16 - 8s - loss: 3.7213 - accuracy: 0.2408 - 8s/epoch - 506ms/step\n",
      "Epoch 30/100\n",
      "16/16 - 8s - loss: 3.6453 - accuracy: 0.2452 - 8s/epoch - 515ms/step\n",
      "Epoch 31/100\n",
      "16/16 - 9s - loss: 3.5429 - accuracy: 0.2664 - 9s/epoch - 566ms/step\n",
      "Epoch 32/100\n",
      "16/16 - 8s - loss: 3.4394 - accuracy: 0.2777 - 8s/epoch - 523ms/step\n",
      "Epoch 33/100\n",
      "16/16 - 8s - loss: 3.3440 - accuracy: 0.2877 - 8s/epoch - 521ms/step\n",
      "Epoch 34/100\n",
      "16/16 - 8s - loss: 3.2450 - accuracy: 0.3158 - 8s/epoch - 526ms/step\n",
      "Epoch 35/100\n",
      "16/16 - 8s - loss: 3.1490 - accuracy: 0.3358 - 8s/epoch - 529ms/step\n",
      "Epoch 36/100\n",
      "16/16 - 8s - loss: 3.0373 - accuracy: 0.3502 - 8s/epoch - 530ms/step\n",
      "Epoch 37/100\n",
      "16/16 - 8s - loss: 2.9499 - accuracy: 0.3652 - 8s/epoch - 528ms/step\n",
      "Epoch 38/100\n",
      "16/16 - 9s - loss: 2.8604 - accuracy: 0.3921 - 9s/epoch - 533ms/step\n",
      "Epoch 39/100\n",
      "16/16 - 9s - loss: 2.7983 - accuracy: 0.4103 - 9s/epoch - 532ms/step\n",
      "Epoch 40/100\n",
      "16/16 - 9s - loss: 2.7170 - accuracy: 0.4296 - 9s/epoch - 537ms/step\n",
      "Epoch 41/100\n",
      "16/16 - 9s - loss: 2.6341 - accuracy: 0.4628 - 9s/epoch - 534ms/step\n",
      "Epoch 42/100\n",
      "16/16 - 9s - loss: 2.5441 - accuracy: 0.4903 - 9s/epoch - 534ms/step\n",
      "Epoch 43/100\n",
      "16/16 - 9s - loss: 2.4764 - accuracy: 0.5128 - 9s/epoch - 542ms/step\n",
      "Epoch 44/100\n",
      "16/16 - 9s - loss: 2.3951 - accuracy: 0.5341 - 9s/epoch - 543ms/step\n",
      "Epoch 45/100\n",
      "16/16 - 9s - loss: 2.3223 - accuracy: 0.5647 - 9s/epoch - 544ms/step\n",
      "Epoch 46/100\n",
      "16/16 - 9s - loss: 2.2558 - accuracy: 0.5835 - 9s/epoch - 544ms/step\n",
      "Epoch 47/100\n",
      "16/16 - 9s - loss: 2.1817 - accuracy: 0.6123 - 9s/epoch - 545ms/step\n",
      "Epoch 48/100\n",
      "16/16 - 9s - loss: 2.1129 - accuracy: 0.6254 - 9s/epoch - 548ms/step\n",
      "Epoch 49/100\n",
      "16/16 - 9s - loss: 2.0512 - accuracy: 0.6517 - 9s/epoch - 550ms/step\n",
      "Epoch 50/100\n",
      "16/16 - 9s - loss: 2.0130 - accuracy: 0.6529 - 9s/epoch - 554ms/step\n",
      "Epoch 51/100\n",
      "16/16 - 9s - loss: 1.9413 - accuracy: 0.6685 - 9s/epoch - 555ms/step\n",
      "Epoch 52/100\n",
      "16/16 - 9s - loss: 1.8600 - accuracy: 0.6942 - 9s/epoch - 572ms/step\n",
      "Epoch 53/100\n",
      "16/16 - 9s - loss: 1.7903 - accuracy: 0.7248 - 9s/epoch - 559ms/step\n",
      "Epoch 54/100\n",
      "16/16 - 9s - loss: 1.7224 - accuracy: 0.7398 - 9s/epoch - 563ms/step\n",
      "Epoch 55/100\n",
      "16/16 - 9s - loss: 1.6681 - accuracy: 0.7473 - 9s/epoch - 571ms/step\n",
      "Epoch 56/100\n",
      "16/16 - 9s - loss: 1.6039 - accuracy: 0.7742 - 9s/epoch - 567ms/step\n",
      "Epoch 57/100\n",
      "16/16 - 9s - loss: 1.5551 - accuracy: 0.7805 - 9s/epoch - 568ms/step\n",
      "Epoch 58/100\n",
      "16/16 - 9s - loss: 1.5008 - accuracy: 0.7992 - 9s/epoch - 569ms/step\n",
      "Epoch 59/100\n",
      "16/16 - 9s - loss: 1.4508 - accuracy: 0.8043 - 9s/epoch - 568ms/step\n",
      "Epoch 60/100\n",
      "16/16 - 9s - loss: 1.3949 - accuracy: 0.8211 - 9s/epoch - 572ms/step\n",
      "Epoch 61/100\n",
      "16/16 - 9s - loss: 1.3480 - accuracy: 0.8343 - 9s/epoch - 570ms/step\n",
      "Epoch 62/100\n",
      "16/16 - 9s - loss: 1.3087 - accuracy: 0.8468 - 9s/epoch - 571ms/step\n",
      "Epoch 63/100\n",
      "16/16 - 9s - loss: 1.2681 - accuracy: 0.8474 - 9s/epoch - 571ms/step\n",
      "Epoch 64/100\n",
      "16/16 - 9s - loss: 1.2280 - accuracy: 0.8487 - 9s/epoch - 581ms/step\n",
      "Epoch 65/100\n",
      "16/16 - 9s - loss: 1.1830 - accuracy: 0.8674 - 9s/epoch - 592ms/step\n",
      "Epoch 66/100\n",
      "16/16 - 9s - loss: 1.1378 - accuracy: 0.8762 - 9s/epoch - 576ms/step\n",
      "Epoch 67/100\n",
      "16/16 - 9s - loss: 1.0991 - accuracy: 0.8849 - 9s/epoch - 572ms/step\n",
      "Epoch 68/100\n",
      "16/16 - 9s - loss: 1.0612 - accuracy: 0.8974 - 9s/epoch - 570ms/step\n",
      "Epoch 69/100\n",
      "16/16 - 9s - loss: 1.0225 - accuracy: 0.8949 - 9s/epoch - 572ms/step\n",
      "Epoch 70/100\n",
      "16/16 - 9s - loss: 0.9882 - accuracy: 0.9031 - 9s/epoch - 575ms/step\n",
      "Epoch 71/100\n",
      "16/16 - 9s - loss: 0.9528 - accuracy: 0.9143 - 9s/epoch - 579ms/step\n",
      "Epoch 72/100\n",
      "16/16 - 9s - loss: 0.9222 - accuracy: 0.9168 - 9s/epoch - 581ms/step\n",
      "Epoch 73/100\n",
      "16/16 - 9s - loss: 0.8912 - accuracy: 0.9256 - 9s/epoch - 574ms/step\n",
      "Epoch 74/100\n",
      "16/16 - 9s - loss: 0.8631 - accuracy: 0.9325 - 9s/epoch - 577ms/step\n",
      "Epoch 75/100\n",
      "16/16 - 9s - loss: 0.8351 - accuracy: 0.9343 - 9s/epoch - 578ms/step\n",
      "Epoch 76/100\n",
      "16/16 - 9s - loss: 0.8069 - accuracy: 0.9362 - 9s/epoch - 582ms/step\n",
      "Epoch 77/100\n",
      "16/16 - 9s - loss: 0.7781 - accuracy: 0.9431 - 9s/epoch - 587ms/step\n",
      "Epoch 78/100\n",
      "16/16 - 9s - loss: 0.7589 - accuracy: 0.9412 - 9s/epoch - 585ms/step\n",
      "Epoch 79/100\n",
      "16/16 - 9s - loss: 0.7290 - accuracy: 0.9450 - 9s/epoch - 581ms/step\n",
      "Epoch 80/100\n",
      "16/16 - 9s - loss: 0.7079 - accuracy: 0.9481 - 9s/epoch - 591ms/step\n",
      "Epoch 81/100\n",
      "16/16 - 9s - loss: 0.8293 - accuracy: 0.9043 - 9s/epoch - 591ms/step\n",
      "Epoch 82/100\n",
      "16/16 - 9s - loss: 0.7853 - accuracy: 0.9093 - 9s/epoch - 582ms/step\n",
      "Epoch 83/100\n",
      "16/16 - 10s - loss: 0.7110 - accuracy: 0.9381 - 10s/epoch - 645ms/step\n",
      "Epoch 84/100\n",
      "16/16 - 10s - loss: 0.6601 - accuracy: 0.9531 - 10s/epoch - 598ms/step\n",
      "Epoch 85/100\n",
      "16/16 - 9s - loss: 0.6285 - accuracy: 0.9543 - 9s/epoch - 594ms/step\n",
      "Epoch 86/100\n",
      "16/16 - 9s - loss: 0.5977 - accuracy: 0.9587 - 9s/epoch - 585ms/step\n",
      "Epoch 87/100\n",
      "16/16 - 10s - loss: 0.5722 - accuracy: 0.9606 - 10s/epoch - 627ms/step\n",
      "Epoch 88/100\n",
      "16/16 - 9s - loss: 0.5503 - accuracy: 0.9625 - 9s/epoch - 587ms/step\n",
      "Epoch 89/100\n",
      "16/16 - 9s - loss: 0.5573 - accuracy: 0.9581 - 9s/epoch - 588ms/step\n",
      "Epoch 90/100\n",
      "16/16 - 9s - loss: 0.5729 - accuracy: 0.9500 - 9s/epoch - 579ms/step\n",
      "Epoch 91/100\n",
      "16/16 - 9s - loss: 0.5470 - accuracy: 0.9525 - 9s/epoch - 583ms/step\n",
      "Epoch 92/100\n",
      "16/16 - 9s - loss: 0.5272 - accuracy: 0.9568 - 9s/epoch - 582ms/step\n",
      "Epoch 93/100\n",
      "16/16 - 9s - loss: 0.5010 - accuracy: 0.9650 - 9s/epoch - 573ms/step\n",
      "Epoch 94/100\n",
      "16/16 - 9s - loss: 0.4781 - accuracy: 0.9669 - 9s/epoch - 568ms/step\n",
      "Epoch 95/100\n",
      "16/16 - 9s - loss: 0.4575 - accuracy: 0.9694 - 9s/epoch - 578ms/step\n",
      "Epoch 96/100\n",
      "16/16 - 9s - loss: 0.4396 - accuracy: 0.9675 - 9s/epoch - 581ms/step\n",
      "Epoch 97/100\n",
      "16/16 - 9s - loss: 0.4248 - accuracy: 0.9694 - 9s/epoch - 587ms/step\n",
      "Epoch 98/100\n",
      "16/16 - 9s - loss: 0.4119 - accuracy: 0.9712 - 9s/epoch - 568ms/step\n",
      "Epoch 99/100\n",
      "16/16 - 9s - loss: 0.3956 - accuracy: 0.9700 - 9s/epoch - 564ms/step\n",
      "Epoch 100/100\n",
      "16/16 - 9s - loss: 0.3839 - accuracy: 0.9712 - 9s/epoch - 563ms/step\n"
     ]
    }
   ],
   "source": [
    "# compile reverse sequence network\n",
    "rev_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "rev_model.fit(rev_X, rev_y,batch_size=100, epochs=100, verbose=2)\n",
    "# save the model to file\n",
    "rev_model.save('rev_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence using a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text):\n",
    "    if seed_text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        in_text = seed_text\n",
    "        n_words = 1\n",
    "        n_preds = 5 #number of words to predict for the seed text\n",
    "        pred_words = \"\"\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            # pre-pad sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "            # predict probabilities for each word\n",
    "            proba = model.predict(encoded, verbose=0).flatten()\n",
    "            #take the n_preds highest probability classes \n",
    "            yhat = numpy.argsort(-proba)[:n_preds] \n",
    "            # map predicted words index to word\n",
    "            out_word = ''\n",
    "\n",
    "            for _ in range(n_preds):\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == yhat[_] and word not in stopwords: # CHECK THIS \n",
    "                        out_word = word\n",
    "                        pred_words += ' ' + out_word\n",
    "                        #print(out_word)\n",
    "                        break\n",
    "\n",
    "\n",
    "        return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "rev_model = load_model('rev_model.h5')\n",
    "\n",
    "#load tokeniser and max_length\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    \n",
    "with open('max_length.pkl', 'rb') as f:\n",
    "    max_length = pickle.load(f)\n",
    "    \n",
    "\n",
    "#load spacy GloVe Model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#loading stopwords to improve relevant word predictions    \n",
    "stopwords= nlp.Defaults.stop_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and set embeddings for OOV words\n",
    "def set_embedding_for_oov(doc):\n",
    "    #checking for oov words and adding embedding\n",
    "    print(type(doc))\n",
    "    j=0 #for keeping track of index to replace\n",
    "    changed_string=\"\"\n",
    "    doc2=doc\n",
    "    for token in doc:\n",
    "        changed_string=changed_string+str(token.text_with_ws)\n",
    "        if token.is_oov == True:\n",
    "\n",
    "            \n",
    "            \n",
    "            before_text = doc[:token.i].text\n",
    "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
    "\n",
    "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
    "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
    "            \n",
    "            embedding = numpy.zeros((300,))\n",
    "\n",
    "            i=len(before_text)\n",
    "            print('Words predicted from forward sequence model:')\n",
    "            for word in pred_before:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            i=len(after_text)\n",
    "            print('Words predicted from reverse sequence model:')\n",
    "            for word in pred_after:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            nlp.vocab.set_vector(token.text, embedding)  #CHECK: So we don't include the shitty words?\n",
    "\n",
    "            \n",
    "        \n",
    "           \n",
    "             \n",
    "            #set vector back to zero?\n",
    "            #nlp.vocab.set_vector(token.text, 0)\n",
    "\n",
    "##########################################################################################################\n",
    "            \n",
    "            #Locate similar words\n",
    "            by_similarity = sorted(token.vocab, key=lambda w: token.similarity(w), reverse=True)\n",
    "            #find the closest word to the token to be a replacement\n",
    "            replace=[w.orth_ for w in by_similarity[:3]][2]\n",
    "            #cast to string\n",
    "            token=str(token)\n",
    "         \n",
    "            \n",
    "            \n",
    "            changed_string=changed_string.replace(token, replace)\n",
    "\n",
    "            #create new doc with replaced word\n",
    "            doc2=nlp.make_doc(changed_string)\n",
    "            \n",
    "            j=j+1\n",
    "\n",
    "        doc2=nlp.make_doc(changed_string)\n",
    "\n",
    "    #assign original document to new document and return original document\n",
    "    doc=doc2    \n",
    "    return changed_string  #return doc if you want document returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"I think that student would benefit from learning attttttt home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortab at home. The hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear. most students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go. when your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class. \"\n",
    ")\n",
    "#print(nlp.vocab.get_vector('studen'))\n",
    "doc=set_embedding_for_oov(doc)\n",
    "#print(nlp.vocab.get_vector('studen'))\n",
    "doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Words predicted from forward sequence model:\n",
      "Words predicted from reverse sequence model:\n",
      "gulf\n",
      "worse\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsedl\\AppData\\Local\\Temp\\ipykernel_21384\\1166202396.py:46: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  by_similarity = sorted(token.vocab, key=lambda w: token.similarity(w), reverse=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words predicted from forward sequence model:\n",
      "you've\n",
      "better\n",
      "Words predicted from reverse sequence model:\n",
      "gulf\n",
      "worse\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "['Miss. Charlie, you are nice', 'hello hellllllllllll, you are nice', 'hello joe, you are stupid']\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "arr= [\"heldifhadkjf Charlie, you are nice\", \"hello lkadhfasldkjfhlkj, you are nice\", \"hello joe, you are stupid\" ]\n",
    "for essay in arr:\n",
    "\n",
    "    doc=nlp(essay)\n",
    "    fixed=set_embedding_for_oov(doc)\n",
    "    arr[index]=fixed\n",
    "    index=index+1\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsedl\\AppData\\Local\\Temp\\ipykernel_21384\\1143453434.py:3: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n"
     ]
    }
   ],
   "source": [
    "lis=most_similar(nlp('comfortab'))[0]\n",
    "print(type(lis))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find most similar words\n",
    "def most_similar(word):\n",
    "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.orth_ for w in by_similarity[:1]] #change :x for x amoint of similar words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

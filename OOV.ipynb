{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "import numpy\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleandata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#reading processed data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcleandata.csv\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()[:\u001b[39m100000\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[39mtype\u001b[39m(data)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleandata.csv'"
     ]
    }
   ],
   "source": [
    "#reading processed data\n",
    "data = open('cleandata.csv').read()[:100000]\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for preparing text data into sequences for training \n",
    "def data_sequencing(data):   \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([data])\n",
    "    with open('tokenizer.pkl', 'wb') as f: # Save the tokeniser by pickling it\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "    # retrieve vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocabulary Size: %d' % vocab_size)\n",
    "    \n",
    "    # create line-based sequences\n",
    "    sequences = list()\n",
    "    rev_sequences = list()\n",
    "    for line in data.split('.'):\n",
    "        encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "        rev_encoded = encoded[::-1]\n",
    "        for i in range(1, len(encoded)):\n",
    "            sequence = encoded[:i+1]\n",
    "            rev_sequence = rev_encoded[:i+1]\n",
    "            sequences.append(sequence)\n",
    "            rev_sequences.append(rev_sequence)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    \n",
    "    #find max sequence length \n",
    "    max_length = max([len(seq) for seq in sequences])\n",
    "    with open('max_length.pkl', 'wb') as f: # Save max_length by pickling it\n",
    "        pickle.dump(max_length, f)\n",
    "    print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "    # pad sequences and create the forward sequence\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    sequences = array(sequences)\n",
    "    X, y = sequences[:,:-1],sequences[:,-1]\n",
    "    \n",
    "    #pad sequences and create the reverse sequencing\n",
    "    rev_sequences = pad_sequences(rev_sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    rev_sequences = array(rev_sequences)\n",
    "    rev_X, rev_y = rev_sequences[:,:-1],rev_sequences[:,-1]\n",
    "\n",
    "    return X,y,rev_X,rev_y,max_length,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning forward and reverse sequences along with max sequence \n",
    "#length from the data \n",
    "\n",
    "X,y,rev_X,rev_y,max_length,vocab_size = data_sequencing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define forward sequence model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,100, input_length=max_length-1))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define reverse model\n",
    "rev_model = Sequential()\n",
    "rev_model.add(Embedding(vocab_size, 100, input_length=max_length-1))\n",
    "#rev_model.add(LSTM(100))\n",
    "rev_model.add(Bidirectional(LSTM(100)))\n",
    "rev_model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(rev_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile forward sequence network\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y,batch_size=100, epochs=200, verbose=2)\n",
    "# save the model to file\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile reverse sequence network\n",
    "rev_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "rev_model.fit(rev_X, rev_y,batch_size=100, epochs=200, verbose=2)\n",
    "# save the model to file\n",
    "rev_model.save('rev_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence using a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text):\n",
    "    if seed_text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        in_text = seed_text\n",
    "        n_words = 1\n",
    "        n_preds = 5 #number of words to predict for the seed text\n",
    "        pred_words = \"\"\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            # pre-pad sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "            # predict probabilities for each word\n",
    "            proba = model.predict(encoded, verbose=0).flatten()\n",
    "            #take the n_preds highest probability classes \n",
    "            yhat = numpy.argsort(-proba)[:n_preds] \n",
    "            # map predicted words index to word\n",
    "            out_word = ''\n",
    "\n",
    "            for _ in range(n_preds):\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == yhat[_] and word not in stopwords:\n",
    "                        out_word = word\n",
    "                        pred_words += ' ' + out_word\n",
    "                        #print(out_word)\n",
    "                        break\n",
    "\n",
    "\n",
    "        return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "rev_model = load_model('rev_model.h5')\n",
    "\n",
    "#load tokeniser and max_length\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    \n",
    "with open('max_length.pkl', 'rb') as f:\n",
    "    max_length = pickle.load(f)\n",
    "    \n",
    "#loading stopwords to improve relevant word predictions    \n",
    "stopwords= open('stopwords').read().split()\n",
    "\n",
    "#load spacy GloVe Model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and set embeddings for OOV words\n",
    "def set_embedding_for_oov(doc):\n",
    "    #checking for oov words and adding embedding\n",
    "    for token in doc:\n",
    "        if token.is_oov == True:\n",
    "            before_text = doc[:token.i].text\n",
    "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
    "\n",
    "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
    "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
    "            \n",
    "            embedding = numpy.zeros((300,))\n",
    "\n",
    "            i=len(before_text)\n",
    "            print('Words predicted from forward sequence model:')\n",
    "            for word in pred_before:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            i=len(after_text)\n",
    "            print('Words predicted from reverse sequence model:')\n",
    "            for word in pred_after:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            nlp.vocab.set_vector(token.text, embedding)\n",
    "            print(token.text,nlp.vocab.get_vector(token.text))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('i livwgffe in london ')\n",
    "set_embedding_for_oov(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(nlp('livwgffe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find most similar words\n",
    "def most_similar(word):\n",
    "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.orth_ for w in by_similarity[:10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

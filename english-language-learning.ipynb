{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF of essays \n",
    "This jupyter notebook takes in a pool of essays, and constructs the TF-IDF score and trains a neural network to take in a TF-IDF matrix and compute a vocabulary score based on this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthis is the way to get kaggle files I think\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "'''\n",
    "this is the way to get kaggle files I think\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "\n",
    "#hello\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv file\n",
    "full_train_data = pd.read_csv('./data/train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in 333,000 most common words in English language, not currently used later in code but might be useful...\n",
    "all_vocab_words = pd.read_csv('./data/unigram_freq.csv', index_col=0)\n",
    "all_vocabs = np.array([data[0] for data in all_vocab_words.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse over every row, store essay in the essays_train/test lists, and store the 6 scores in the scores_train/test list\n",
    "essays_train = []\n",
    "scores_train = []\n",
    "essays_test = []\n",
    "scores_test = []\n",
    "all_essays = []\n",
    "all_vocabs_words = []\n",
    "\n",
    "#Technique for splitting essays into training and testing data and results\n",
    "'''\n",
    "for ind, data in enumerate(full_train_data.iterrows()):\n",
    "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
    "    essays_train.append(text) if ind % 2 == 0 else essays_test.append(text)\n",
    "    scores_train.append([cohes, syntax, vocab, phrase, gram, convs]) if ind % 2 == 0 else scores_test.append(\n",
    "        [cohes, syntax, vocab, phrase, gram, convs]) '''\n",
    "\n",
    "for ind, data in enumerate(full_train_data.iterrows()):\n",
    "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
    "    all_essays.append(text)\n",
    "    all_vocabs_words.append(vocab) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make into np arrays, currently not used\n",
    "np_essays_train = np.array(essays_train)\n",
    "np_essays_test = np.array(essays_test)\n",
    "np_scores_train = np.array(scores_train)\n",
    "np_scores_test = np.array(scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tfIdf for all essays in bank\n",
    "#np_all_essays = np.concatenate((essays_train, essays_test), axis = 0)\n",
    "np_all_essays = np.array(all_essays)\n",
    "np_all_scores = np.array(all_vocabs_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement tf-idf over all essays\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(np_all_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split tf-idf vectors and associated vocab scores into a training and testing \n",
    "#data set\n",
    "embedded_training = []\n",
    "embedded_testing = []\n",
    "score_training = []\n",
    "score_testing = []\n",
    "\n",
    "for index, essay in enumerate(np_all_essays):\n",
    "  tf_idf_per_word = tfIdf[index].toarray()[0]\n",
    "  essay_score = np_all_scores[index]\n",
    "  if index % 2 == 0:\n",
    "    embedded_training.append(tf_idf_per_word)\n",
    "    score_training.append(essay_score)\n",
    "  else:\n",
    "    embedded_testing.append(tf_idf_per_word)\n",
    "    score_testing.append(essay_score)\n",
    "\n",
    "np_embedded_training = np.array(embedded_training)\n",
    "np_embedded_testing = np.array(embedded_testing)\n",
    "np_score_training= np.array(score_training)\n",
    "np_score_testing = np.array(score_testing)\n",
    "\n",
    "#currently unused code\n",
    "\n",
    "#df = pd.DataFrame(tfIdf[index].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "#tf_idf_per_word = np.array([data[1] for data in df.iterrows()])\n",
    "#indices=np.array(tfIdfVectorizer.get_feature_names())\n",
    "#df = df.sort_values('TF-IDF', ascending=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word to index and index to word mapping to make looking up values easy\n",
    "indices=np.array(tfIdfVectorizer.get_feature_names())\n",
    "ind_to_word = {} \n",
    "word_to_ind = {}\n",
    "for index, word in enumerate(indices):\n",
    "  ind_to_word[index] = word\n",
    "  word_to_ind[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\\ndf = df.sort_values(\\'TF-IDF\\', ascending=False)\\nprint (df.head(50))'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(50))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**:\n",
    "To reverse engineer TF-IDF, SKlearn does a few things different.\n",
    "\n",
    "- TF is calculated by # of times word appears in document.\n",
    "- IDF is calculated by log base e ([1 + # documents] / [1 + # documents word appears in])\n",
    "\n",
    "Once all the words have TF-IDF scores, they are normalized by dividing by the square of all the other vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will be useful later\n",
    "idf_vector_for_every_word = tfIdfVectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of distinct words, and therefore our input layer\n",
    "num_vars = len(indices)\n",
    "\n",
    "#at this point, we have a training set of TF-IDF vectors and a testing set, \n",
    "# as well as the associated scores. next step is to create a NN to connect them\n",
    "#  and create weights\n",
    "\n",
    "# after the NN is created, we will need to take an essay and convert it into a \n",
    "# TF-IDF vector over the same word bank, and compute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use LSTMS on word embeddings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

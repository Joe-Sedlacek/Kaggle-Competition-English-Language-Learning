{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqtDVhuHBGFT"
      },
      "source": [
        "# **Vocabulary Score of Essays**\n",
        "This jupyter notebook takes in a pool of essays and goes through 2 approaches of \n",
        "creating a Neural Network to get the vocabulary score for every essay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "I-XDiuc8BGFV",
        "outputId": "a01392d7-7310-4b51-97d7-2eaeacbc4c3f"
      },
      "outputs": [],
      "source": [
        "# Import statements \n",
        "\n",
        "import re\n",
        "import math\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZmO3V5XHBGFX"
      },
      "outputs": [],
      "source": [
        "#read in csv file\n",
        "full_train_data = pd.read_csv('./data/train.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Approach #1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U4iO1TCaBGFb"
      },
      "outputs": [],
      "source": [
        "#parse over every row, store essay in the essays_train/test lists, and store the 6 scores in the scores_train/test list\n",
        "essays_train = []\n",
        "scores_train = []\n",
        "essays_test = []\n",
        "scores_test = []\n",
        "\n",
        "#Splitting data into training and testing subsets\n",
        "\n",
        "for ind, data in enumerate(full_train_data.iterrows()):\n",
        "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
        "    if ind % 5 == 0:\n",
        "      essays_test.append(text) \n",
        "      scores_test.append(vocab)\n",
        "    else:\n",
        "      essays_train.append(text)\n",
        "      scores_train.append(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#make into np arrays\n",
        "np_essays_train = np.array(essays_train)\n",
        "np_essays_test = np.array(essays_test)\n",
        "np_scores_train = np.array(scores_train)\n",
        "np_scores_test = np.array(scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Implement TF-IDF over all essays\n",
        "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "tfIdf = tfIdfVectorizer.fit_transform(np_essays_train)\n",
        "idf_vector_for_every_word = tfIdfVectorizer.idf_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jjPz__hJBGFb"
      },
      "outputs": [],
      "source": [
        "#create word to index and index to word mapping to make looking up values easy\n",
        "indices=np.array(tfIdfVectorizer.get_feature_names())\n",
        "ind_to_word = {} \n",
        "word_to_ind = {}\n",
        "for index, word in enumerate(indices):\n",
        "  ind_to_word[index] = word\n",
        "  word_to_ind[word] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Building the array of TF-IDF vectors for the training data\n",
        "tfidfs_train = []\n",
        "\n",
        "for index, essay in enumerate(np_essays_train):\n",
        "  tfidfs_train.append(tfIdf[index].toarray()[0])\n",
        "\n",
        "np_tfidfs_train = np.array(tfidfs_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TyT1cKc0BGFd"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Dense(2000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['mean_absolute_error'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO6Ux8wiBUUA",
        "outputId": "a95d1546-33ab-4a69-8f39-9f04060b4f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 0.7307 - mean_absolute_error: 0.6365\n",
            "Epoch 2/20\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 0.3630 - mean_absolute_error: 0.4759\n",
            "Epoch 3/20\n",
            "196/196 [==============================] - 20s 100ms/step - loss: 0.2953 - mean_absolute_error: 0.4312\n",
            "Epoch 4/20\n",
            "196/196 [==============================] - 25s 128ms/step - loss: 0.2538 - mean_absolute_error: 0.3975\n",
            "Epoch 5/20\n",
            "196/196 [==============================] - 26s 135ms/step - loss: 0.1925 - mean_absolute_error: 0.3441\n",
            "Epoch 6/20\n",
            "196/196 [==============================] - 26s 134ms/step - loss: 0.1550 - mean_absolute_error: 0.3096\n",
            "Epoch 7/20\n",
            "196/196 [==============================] - 19s 98ms/step - loss: 0.1349 - mean_absolute_error: 0.2874\n",
            "Epoch 8/20\n",
            "196/196 [==============================] - 28s 141ms/step - loss: 0.1121 - mean_absolute_error: 0.2605\n",
            "Epoch 9/20\n",
            "196/196 [==============================] - 27s 139ms/step - loss: 0.0973 - mean_absolute_error: 0.2425\n",
            "Epoch 10/20\n",
            "196/196 [==============================] - 27s 137ms/step - loss: 0.0824 - mean_absolute_error: 0.2240\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 24s 122ms/step - loss: 0.0724 - mean_absolute_error: 0.2042\n",
            "Epoch 12/20\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.0606 - mean_absolute_error: 0.1906\n",
            "Epoch 13/20\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.0571 - mean_absolute_error: 0.1820\n",
            "Epoch 14/20\n",
            "196/196 [==============================] - 18s 94ms/step - loss: 0.0525 - mean_absolute_error: 0.1736\n",
            "Epoch 15/20\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 0.0437 - mean_absolute_error: 0.1585\n",
            "Epoch 16/20\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.0404 - mean_absolute_error: 0.1511\n",
            "Epoch 17/20\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.0411 - mean_absolute_error: 0.1498\n",
            "Epoch 18/20\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 0.0388 - mean_absolute_error: 0.1455\n",
            "Epoch 19/20\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 0.0352 - mean_absolute_error: 0.1388\n",
            "Epoch 20/20\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.0298 - mean_absolute_error: 0.1276\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x162a89580>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(np_tfidfs_train, np_scores_train, batch_size=16, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Building the array of TF-IDF vectors for the testing data\n",
        "tf_len = len(idf_vector_for_every_word)\n",
        "tfidfs_test = []\n",
        "unknown_words = []\n",
        "\n",
        "for index, essay in enumerate(np_essays_test):\n",
        "  essay_wo_punc = re.sub(r'[^\\w\\s]', '', essay)\n",
        "  essay_lower = essay_wo_punc.lower()\n",
        "  split_essay = re.split('[^a-zA-Z]+', essay_lower)\n",
        "  tf = [0 for _ in range(tf_len)]\n",
        "  for word in split_essay:\n",
        "    if word in word_to_ind:\n",
        "      tf[word_to_ind[word]] += 1\n",
        "    else:\n",
        "      unknown_words.append(word)\n",
        "  tfIdf = np.array(tf) * idf_vector_for_every_word\n",
        "  np_tfidf_not_normal = np.array(tfIdf)\n",
        "  tfIdf_norm = np_tfidf_not_normal/np.linalg.norm(np_tfidf_not_normal)\n",
        "  tfidfs_test.append(tfIdf_norm)\n",
        "\n",
        "np_tfidfs_test = np.array(tfidfs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P0R8I_6rMA5",
        "outputId": "10c2579f-ccc5-4aa6-b0f5-aacb06d09122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 1s 25ms/step - loss: 0.2839 - mean_absolute_error: 0.4110\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.2838650345802307, 0.4110131561756134]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Evaluate success of model\n",
        "model.evaluate(np_tfidfs_test, np_scores_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Approach #2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KdrgQ7GGBGFY"
      },
      "outputs": [],
      "source": [
        "#Reads in 333,000 most common words in English language, not currently used later in code but might be useful...\n",
        "all_vocab_words = pd.read_csv('./data/unigram_freq.csv', index_col=0)\n",
        "all_vocabs = np.array([data[0] for data in all_vocab_words.iterrows()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qf40-kLDBGFY"
      },
      "outputs": [],
      "source": [
        "#parse over every row, store essay in the essays_train/test lists, and store the 6 scores in the scores_train/test list\n",
        "essays_train = []\n",
        "scores_train = []\n",
        "essays_test = []\n",
        "scores_test = []\n",
        "all_essays = []\n",
        "all_vocabs_words = []\n",
        "\n",
        "#Technique for splitting essays into training and testing data and results\n",
        "\n",
        "for ind, data in enumerate(full_train_data.iterrows()):\n",
        "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
        "    essays_train.append(text) if ind % 5 != 0 else essays_test.append(text)\n",
        "    scores_train.append([cohes, syntax, vocab, phrase, gram, convs]) if ind % 5 != 0 else scores_test.append(\n",
        "        [cohes, syntax, vocab, phrase, gram, convs]) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FzmhtBA2BGFY"
      },
      "outputs": [],
      "source": [
        "#make into np arrays\n",
        "np_essays_train = np.array(essays_train)\n",
        "np_essays_test = np.array(essays_test)\n",
        "np_scores_train = np.array(scores_train)\n",
        "np_scores_test = np.array(scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# building dictionary with vocab word as key and count as value\n",
        "vocab_dict = {}\n",
        "\n",
        "for data in all_vocab_words[:50001].iterrows():\n",
        "  vocab_dict[data[0]] = data[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get frequency of most common word, 'the' to use to do frequency\n",
        "normalizer_frequency = vocab_dict[\"the\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create vocab to index and index to vocab mappings\n",
        "vocab_word_to_index = {}\n",
        "index_to_vocab_word = {}\n",
        "for index, word in enumerate(vocab_dict):\n",
        "  vocab_word_to_index[word] = index\n",
        "  index_to_vocab_word[index] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create idf score for every value\n",
        "our_idf = []\n",
        "#create idf for every word:\n",
        "for word, amount in vocab_dict.items():\n",
        "  idf_score = math.log((normalizer_frequency + 1)/(amount + 1))\n",
        "  our_idf.append(idf_score)\n",
        "\n",
        "np_idf = np.array(our_idf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Parse through every training essay and remove punction and make every word lower case. \n",
        "#Take every word not in most common 50,000 words and add to list of unknown words. \n",
        "#Create TF list for every essay\n",
        "unknown_words = []\n",
        "train_tf_essays = []\n",
        "total_word_count = 0\n",
        "for essay in np_essays_train:\n",
        "  essay_wo_punc = re.sub(r'[^\\w\\s]', '', essay)\n",
        "  essay_lower = essay_wo_punc.lower()\n",
        "  total_word_count += len(split_essay)\n",
        "  split_essay = re.split('[^a-zA-Z]+', essay_lower)\n",
        "  tf = [0 for _ in range(50000)]\n",
        "  #calculate TF Score\n",
        "  for word in split_essay:\n",
        "    if word in vocab_dict:\n",
        "      tf[vocab_word_to_index[word]] += 1\n",
        "    else:\n",
        "      unknown_words.append(word)\n",
        "  train_tf_essays.append(tf)\n",
        "np_train_tf_essays = np.array(train_tf_essays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Parse through every testing essay and remove punction and make every word lower case. \n",
        "#Take every word not in most common 50,000 words and add to list of unknown words. \n",
        "#Create TF list for every essay\n",
        "test_tf_essays = []\n",
        "for essay in np_essays_test:\n",
        "  essay_wo_punc = re.sub(r'[^\\w\\s]', '', essay)\n",
        "  essay_lower = essay_wo_punc.lower()\n",
        "  split_essay = re.split('[^a-zA-Z]+', essay_lower)\n",
        "  total_word_count += len(split_essay)\n",
        "  tf = [0 for _ in range(50000)]\n",
        "  #calculate TF Score\n",
        "  for word in split_essay:\n",
        "    if word in vocab_dict:\n",
        "      tf[vocab_word_to_index[word]] += 1\n",
        "    else:\n",
        "      unknown_words.append(word)\n",
        "  test_tf_essays.append(tf)\n",
        "np_test_tf_essays = np.array(test_tf_essays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ratio of mispelled/unknown words: 0.01647609883280423\n"
          ]
        }
      ],
      "source": [
        "#interesting data stat\n",
        "print(\"Ratio of mispelled/unknown words:\", len(unknown_words)/total_word_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3128\n",
            "783\n"
          ]
        }
      ],
      "source": [
        "#confirm that datasets are the right size (.8 vs. .2)\n",
        "print(len(train_tf_essays))\n",
        "print(len(test_tf_essays))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "50000\n"
          ]
        }
      ],
      "source": [
        "#confirm that tf and idf vectors are equal size\n",
        "print(len(np_train_tf_essays[0]))\n",
        "print(len(np_idf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create modified TF-IDF scores for every training essay\n",
        "tf_idf_per_essay = np_train_tf_essays*np_idf\n",
        "tf_idf_norm_train = tf_idf_per_essay/np.linalg.norm(tf_idf_per_essay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create modified TF-IDF scores for every testing essay\n",
        "tf_idf_per_essay_test = np_test_tf_essays*np_idf\n",
        "tf_idf_norm_test = tf_idf_per_essay_test/np.linalg.norm(tf_idf_per_essay_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Build Neural Network\n",
        "model_app_2 = Sequential([\n",
        "    Dense(2000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model_app_2.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "196/196 [==============================] - 38s 187ms/step - loss: 0.8341 - accuracy: 0.0027\n",
            "Epoch 2/20\n",
            "196/196 [==============================] - 38s 196ms/step - loss: 0.4417 - accuracy: 0.0027\n",
            "Epoch 3/20\n",
            "196/196 [==============================] - 35s 180ms/step - loss: 0.3891 - accuracy: 0.0027\n",
            "Epoch 4/20\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 0.3716 - accuracy: 0.0027\n",
            "Epoch 5/20\n",
            "196/196 [==============================] - 38s 192ms/step - loss: 0.3408 - accuracy: 0.0027\n",
            "Epoch 6/20\n",
            "196/196 [==============================] - 36s 182ms/step - loss: 0.3326 - accuracy: 0.0027\n",
            "Epoch 7/20\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 0.3203 - accuracy: 0.0027\n",
            "Epoch 8/20\n",
            "196/196 [==============================] - 34s 173ms/step - loss: 0.3086 - accuracy: 0.0027\n",
            "Epoch 9/20\n",
            "196/196 [==============================] - 39s 198ms/step - loss: 0.2812 - accuracy: 0.0027\n",
            "Epoch 10/20\n",
            "196/196 [==============================] - 37s 189ms/step - loss: 0.2883 - accuracy: 0.0027\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 36s 182ms/step - loss: 0.2690 - accuracy: 0.0027\n",
            "Epoch 12/20\n",
            "196/196 [==============================] - 34s 174ms/step - loss: 0.2677 - accuracy: 0.0027\n",
            "Epoch 13/20\n",
            "196/196 [==============================] - 34s 174ms/step - loss: 0.2542 - accuracy: 0.0027\n",
            "Epoch 14/20\n",
            "196/196 [==============================] - 34s 173ms/step - loss: 0.2462 - accuracy: 0.0027\n",
            "Epoch 15/20\n",
            "196/196 [==============================] - 35s 181ms/step - loss: 0.2429 - accuracy: 0.0027\n",
            "Epoch 16/20\n",
            "196/196 [==============================] - 34s 173ms/step - loss: 0.2447 - accuracy: 0.0027\n",
            "Epoch 17/20\n",
            "196/196 [==============================] - 34s 171ms/step - loss: 0.2377 - accuracy: 0.0027\n",
            "Epoch 18/20\n",
            "196/196 [==============================] - 34s 173ms/step - loss: 0.2400 - accuracy: 0.0027\n",
            "Epoch 19/20\n",
            "196/196 [==============================] - 35s 178ms/step - loss: 0.2373 - accuracy: 0.0027\n",
            "Epoch 20/20\n",
            "196/196 [==============================] - 34s 175ms/step - loss: 0.2212 - accuracy: 0.0027\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1641474f0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Train model with normalized tf-idf training data, and associated vocab scores\n",
        "model_app_2.fit(tf_idf_norm_train, np_scores_train, batch_size=16, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 2s 59ms/step - loss: 0.8631 - accuracy: 0.0013\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.8630574345588684, 0.0012771391775459051]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#evaluate model with normalized tf-idf testing data and associated testing \n",
        "#vocabulary scores\n",
        "model_app_2.evaluate(tf_idf_norm_test, np_scores_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **OLD Code Below - IGNORE, here for archiving purposes**\n",
        "Most of this code was previous iterations of testing, and is not used in our solutions, but here in case we want to look back on the model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 66s 518ms/step - loss: 3.1639 - accuracy: 0.0032\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 63s 509ms/step - loss: 1.5439 - accuracy: 0.0032\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 61s 492ms/step - loss: 1.2034 - accuracy: 0.0032\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 58s 468ms/step - loss: 0.7640 - accuracy: 0.0032\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 60s 488ms/step - loss: 0.5917 - accuracy: 0.0032\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 58s 470ms/step - loss: 0.4306 - accuracy: 0.0032\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 57s 463ms/step - loss: 0.3928 - accuracy: 0.0032\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 57s 462ms/step - loss: 0.3810 - accuracy: 0.0032\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 57s 462ms/step - loss: 0.3565 - accuracy: 0.0032\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 58s 468ms/step - loss: 0.3465 - accuracy: 0.0032\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 58s 471ms/step - loss: 0.3204 - accuracy: 0.0032\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 58s 474ms/step - loss: 0.3103 - accuracy: 0.0032\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 57s 462ms/step - loss: 0.3046 - accuracy: 0.0032\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 58s 468ms/step - loss: 0.2839 - accuracy: 0.0032\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 57s 464ms/step - loss: 0.2876 - accuracy: 0.0032\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 56s 456ms/step - loss: 0.2733 - accuracy: 0.0032\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 57s 460ms/step - loss: 0.2725 - accuracy: 0.0032\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 57s 460ms/step - loss: 0.2755 - accuracy: 0.0032\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 57s 461ms/step - loss: 0.2677 - accuracy: 0.0032\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 56s 458ms/step - loss: 0.2498 - accuracy: 0.0032\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15c7229a0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#old code\n",
        "#model2.fit(tf_idf_per_essay, np_scores_train, batch_size=16, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 6s 93ms/step - loss: 0.3255 - accuracy: 0.0015\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.32551485300064087, 0.0015345268184319139]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.evaluate(tf_idf_per_essay_test, np_scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.save(\"common_words_tf-idf_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KUjjLyFBGFd"
      },
      "source": [
        "- use LSTMS on word embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3 = Sequential([\n",
        "    Dense(2000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 24.        ,   3.54098918,  48.55919941, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 32.        ,  14.16395673,  10.40554273, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 24.        ,  31.86890264,   6.93702849, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       ...,\n",
              "       [ 12.        ,   0.        ,  24.2795997 , ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 16.        ,  10.62296755,  13.87405697, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [108.        ,  10.62296755,  97.11839881, ...,   0.        ,\n",
              "          0.        ,   0.        ]])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf_per_essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 6s 255ms/step - loss: 3.0916 - accuracy: 0.0028\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 5s 250ms/step - loss: 1.9745 - accuracy: 0.0032\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 5s 252ms/step - loss: 1.4483 - accuracy: 0.0032\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 5s 256ms/step - loss: 1.0356 - accuracy: 0.0032\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 5s 258ms/step - loss: 0.8050 - accuracy: 0.0032\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 5s 262ms/step - loss: 0.6391 - accuracy: 0.0032\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 5s 265ms/step - loss: 0.5174 - accuracy: 0.0032\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.4410 - accuracy: 0.0032\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.3999 - accuracy: 0.0032\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.3472 - accuracy: 0.0032\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.3573 - accuracy: 0.0032\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.3255 - accuracy: 0.0032\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.3035 - accuracy: 0.0032\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.2775 - accuracy: 0.0032\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2782 - accuracy: 0.0032\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 5s 274ms/step - loss: 0.2787 - accuracy: 0.0032\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2724 - accuracy: 0.0032\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2531 - accuracy: 0.0032\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 5s 273ms/step - loss: 0.2465 - accuracy: 0.0032\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2416 - accuracy: 0.0032\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2391 - accuracy: 0.0032\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2301 - accuracy: 0.0032\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.2270 - accuracy: 0.0032\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2195 - accuracy: 0.0032\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2225 - accuracy: 0.0032\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.2197 - accuracy: 0.0032\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2159 - accuracy: 0.0032\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.2267 - accuracy: 0.0032\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.2162 - accuracy: 0.0032\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 5s 265ms/step - loss: 0.2142 - accuracy: 0.0032\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 6s 278ms/step - loss: 0.2110 - accuracy: 0.0032\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2117 - accuracy: 0.0032\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2181 - accuracy: 0.0032\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2100 - accuracy: 0.0032\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2015 - accuracy: 0.0032\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.2086 - accuracy: 0.0032\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.2064 - accuracy: 0.0032\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.1960 - accuracy: 0.0032\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.1947 - accuracy: 0.0032\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.1953 - accuracy: 0.0032\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 5s 262ms/step - loss: 0.1996 - accuracy: 0.0032\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1951 - accuracy: 0.0032\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.1901 - accuracy: 0.0032\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1884 - accuracy: 0.0032\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1895 - accuracy: 0.0032\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1892 - accuracy: 0.0032\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1842 - accuracy: 0.0032\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1830 - accuracy: 0.0032\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1786 - accuracy: 0.0032\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1846 - accuracy: 0.0032\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15c3b8ee0>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.fit(tf_idf_per_essay, np_scores_train, batch_size=100, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 3s 49ms/step - loss: 0.4011 - accuracy: 0.0015\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4010857343673706, 0.0015345268184319139]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.evaluate(tf_idf_per_essay_test, np_scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3.save(\"common_words_tf-idf_new_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_idf_per_essay "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

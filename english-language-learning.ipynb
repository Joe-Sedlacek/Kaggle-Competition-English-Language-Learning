{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqtDVhuHBGFT"
      },
      "source": [
        "# TF-IDF of essays \n",
        "This jupyter notebook takes in a pool of essays, and constructs the TF-IDF score and trains a neural network to take in a TF-IDF matrix and compute a vocabulary score based on this information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "I-XDiuc8BGFV",
        "outputId": "a01392d7-7310-4b51-97d7-2eaeacbc4c3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nthis is the way to get kaggle files I think\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import math\n",
        "import re\n",
        "'''\n",
        "this is the way to get kaggle files I think\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "'''\n",
        "\n",
        "#hello\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZmO3V5XHBGFX"
      },
      "outputs": [],
      "source": [
        "#read in csv file\n",
        "full_train_data = pd.read_csv('./data/train.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qf40-kLDBGFY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor ind, data in enumerate(full_train_data.iterrows()):\\n    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\\n    all_essays.append(text)\\n    all_vocabs_words.append(vocab) '"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#parse over every row, store essay in the essays_train/test lists, and store the 6 scores in the scores_train/test list\n",
        "essays_train = []\n",
        "scores_train = []\n",
        "essays_test = []\n",
        "scores_test = []\n",
        "all_essays = []\n",
        "all_vocabs_words = []\n",
        "\n",
        "#Technique for splitting essays into training and testing data and results\n",
        "\n",
        "for ind, data in enumerate(full_train_data.iterrows()):\n",
        "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
        "    essays_train.append(text) if ind % 2 == 0 else essays_test.append(text)\n",
        "    scores_train.append([cohes, syntax, vocab, phrase, gram, convs]) if ind % 2 == 0 else scores_test.append(\n",
        "        [cohes, syntax, vocab, phrase, gram, convs]) \n",
        "'''\n",
        "for ind, data in enumerate(full_train_data.iterrows()):\n",
        "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
        "    all_essays.append(text)\n",
        "    all_vocabs_words.append(vocab) '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FzmhtBA2BGFY"
      },
      "outputs": [],
      "source": [
        "#make into np arrays, currently not used\n",
        "np_essays_train = np.array(essays_train)\n",
        "np_essays_test = np.array(essays_test)\n",
        "np_scores_train = np.array(scores_train)\n",
        "np_scores_test = np.array(scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LdjAvuUNBGFa"
      },
      "outputs": [],
      "source": [
        "#make tfIdf for all essays in bank\n",
        "#np_all_essays = np.concatenate((essays_train, essays_test), axis = 0)\n",
        "# not use in test approach\n",
        "np_all_essays = np.array(all_essays)\n",
        "np_all_scores = np.array(all_vocabs_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test code new approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KdrgQ7GGBGFY"
      },
      "outputs": [],
      "source": [
        "#Reads in 333,000 most common words in English language, not currently used later in code but might be useful...\n",
        "all_vocab_words = pd.read_csv('./data/unigram_freq.csv', index_col=0)\n",
        "all_vocabs = np.array([data[0] for data in all_vocab_words.iterrows()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# building dictionary with vocab word as key and count as value\n",
        "vocab_dict = {}\n",
        "\n",
        "for data in all_vocab_words[:50001].iterrows():\n",
        "  vocab_dict[data[0]] = data[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23135851162"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_dict[\"the\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalizer_frequency = vocab_dict[\"the\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_word_to_index = {}\n",
        "index_to_vocab_word = {}\n",
        "for index, word in enumerate(vocab_dict):\n",
        "  vocab_word_to_index[word] = index\n",
        "  index_to_vocab_word[index] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "our_idf = []\n",
        "#create idf for every word:\n",
        "for word, amount in vocab_dict.items():\n",
        "  idf_score = math.log((normalizer_frequency + 1)/amount)\n",
        "  our_idf.append(idf_score)\n",
        "\n",
        "np_idf = np.array(our_idf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "unknown_words = []\n",
        "train_tf_essays = []\n",
        "for essay in np_essays_train:\n",
        "  essay_wo_punc = re.sub(r'[^\\w\\s]', '', essay)\n",
        "  essay_lower = essay_wo_punc.lower()\n",
        "  split_essay = re.split('[^a-zA-Z]+', essay_lower)\n",
        "  tf = [0 for _ in range(50000)]\n",
        "  #calculate TF-IDF Score\n",
        "  for word in split_essay:\n",
        "    if word in vocab_dict:\n",
        "      tf[vocab_word_to_index[word]] += 1\n",
        "    else:\n",
        "      unknown_words.append(word)\n",
        "  train_tf_essays.append(tf)\n",
        "np_train_tf_essays = np.array(train_tf_essays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tf_essays = []\n",
        "for essay in np_essays_test:\n",
        "  essay_wo_punc = re.sub(r'[^\\w\\s]', '', essay)\n",
        "  essay_lower = essay_wo_punc.lower()\n",
        "  split_essay = re.split('[^a-zA-Z]+', essay_lower)\n",
        "  tf = [0 for _ in range(50000)]\n",
        "  #calculate TF-IDF Score\n",
        "  for word in split_essay:\n",
        "    if word in vocab_dict:\n",
        "      tf[vocab_word_to_index[word]] += 1\n",
        "    else:\n",
        "      unknown_words.append(word)\n",
        "  test_tf_essays.append(tf)\n",
        "np_test_tf_essays = np.array(test_tf_essays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['homebecause',\n",
              " 'theyll',\n",
              " '',\n",
              " 'aint',\n",
              " 'kindess',\n",
              " 'homeles',\n",
              " 'triying',\n",
              " 'greate',\n",
              " 'kindess',\n",
              " 'wount',\n",
              " 'thoose',\n",
              " 'kindess',\n",
              " 'kindess',\n",
              " 'trowing',\n",
              " 'kindess',\n",
              " '',\n",
              " '',\n",
              " 'isent',\n",
              " 'isyou',\n",
              " 'beatifull',\n",
              " 'isent',\n",
              " 'selfcondifence',\n",
              " 'impotently',\n",
              " 'selfcondience',\n",
              " 'selfcondicence',\n",
              " 'fild',\n",
              " 'selfcondidence',\n",
              " 'negitive',\n",
              " 'incuage',\n",
              " 'disagreedlooking',\n",
              " 'highschol',\n",
              " '',\n",
              " 'grocerys',\n",
              " 'thenthey',\n",
              " 'intead',\n",
              " 'aruge',\n",
              " 'techbology',\n",
              " 'postive',\n",
              " 'thngs',\n",
              " '',\n",
              " 'coronavirus',\n",
              " 'expensives',\n",
              " 'activitys',\n",
              " 'kahootquizlet',\n",
              " '',\n",
              " 'costumer',\n",
              " 'costumer',\n",
              " 'costumer',\n",
              " '',\n",
              " 'pedophiles',\n",
              " '',\n",
              " 'interducaions',\n",
              " 'fanfeeling',\n",
              " 'enjoble',\n",
              " 'consecrate',\n",
              " 'anther',\n",
              " 'paragargh',\n",
              " '',\n",
              " 'einsteins',\n",
              " 'einsteins',\n",
              " 'weve',\n",
              " 'einsteins',\n",
              " 'hypothesize',\n",
              " 'einsteins',\n",
              " 'weve',\n",
              " 'weve',\n",
              " '',\n",
              " 'insted',\n",
              " 'tradional',\n",
              " 'learnd',\n",
              " 'afther',\n",
              " 'afther',\n",
              " 'afther',\n",
              " 'restaurent',\n",
              " 'clases',\n",
              " 'insted',\n",
              " 'clases',\n",
              " 'enought',\n",
              " 'learnd',\n",
              " 'studnet',\n",
              " 'differnt',\n",
              " 'yearsmay',\n",
              " 'laern',\n",
              " 'differenly',\n",
              " 'anothers',\n",
              " 'disavanges',\n",
              " 'insted',\n",
              " 'secand',\n",
              " 'felling',\n",
              " 'prechers',\n",
              " 'enought',\n",
              " 'callage',\n",
              " 'butfor',\n",
              " 'differentin',\n",
              " 'prepere',\n",
              " 'whow',\n",
              " 'callage',\n",
              " 'bitween',\n",
              " 'callage',\n",
              " 'afther',\n",
              " 'fourty',\n",
              " 'anothers',\n",
              " 'responsability',\n",
              " 'sothis',\n",
              " 'inconclusion',\n",
              " 'opinon',\n",
              " 'callage',\n",
              " 'suppor',\n",
              " '',\n",
              " 'insted',\n",
              " 'borther',\n",
              " '',\n",
              " '',\n",
              " 'anyones',\n",
              " 'cannot',\n",
              " 'honestys',\n",
              " 'smartness',\n",
              " 'smartness',\n",
              " 'reteach',\n",
              " 'aint',\n",
              " 'cuss',\n",
              " 'aint',\n",
              " 'aint',\n",
              " 'aint',\n",
              " '',\n",
              " 'differnet',\n",
              " 'opions',\n",
              " 'especiallly',\n",
              " 'foodculture',\n",
              " '',\n",
              " 'auther',\n",
              " 'masterd',\n",
              " 'masted',\n",
              " 'drving',\n",
              " 'gole',\n",
              " 'youy',\n",
              " 'licens',\n",
              " 'challeged',\n",
              " 'fusterated',\n",
              " 'importent',\n",
              " 'hared',\n",
              " 'beacuase',\n",
              " 'callege',\n",
              " 'getteing',\n",
              " 'chaleges',\n",
              " 'challged',\n",
              " 'challeges',\n",
              " 'whay',\n",
              " 'bravest',\n",
              " 'prouder',\n",
              " 'wendys',\n",
              " 'selfesstem',\n",
              " 'esstem',\n",
              " 'esstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'wobbly',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'selfesstem',\n",
              " 'unquestionly',\n",
              " 'selfesstem',\n",
              " 'achivement',\n",
              " 'learining',\n",
              " 'chater',\n",
              " 'selfesstem',\n",
              " '',\n",
              " 'dreamspassion',\n",
              " 'singdancewritereadtechnology',\n",
              " 'sciencemath',\n",
              " 'historyenglish',\n",
              " 'dramaor',\n",
              " 'musicdance',\n",
              " 'technologycarsand',\n",
              " 'dreamssecond',\n",
              " 'areand',\n",
              " 'tings',\n",
              " 'boringsleeping',\n",
              " 'afters',\n",
              " 'opinionsome',\n",
              " 'conmpany',\n",
              " 'empacking',\n",
              " 'anothers',\n",
              " 'everthing',\n",
              " 'promisse',\n",
              " 'foccused',\n",
              " 'empacking',\n",
              " 'anothers',\n",
              " 'hace',\n",
              " '',\n",
              " 'sucess',\n",
              " 'wouldve',\n",
              " 'handlle',\n",
              " 'wouldve',\n",
              " 'improe',\n",
              " 'gades',\n",
              " 'sucessced',\n",
              " 'spead',\n",
              " 'sucessced',\n",
              " '',\n",
              " 'impressons',\n",
              " 'hisher',\n",
              " 'hisher',\n",
              " 'impresses',\n",
              " 'hisher',\n",
              " 'sciencemath',\n",
              " 'sportsarts',\n",
              " 'familiys',\n",
              " 'hisher',\n",
              " 'impressons',\n",
              " 'keypoint',\n",
              " 'hisher',\n",
              " 'hisher',\n",
              " '',\n",
              " 'publicprivate',\n",
              " 'oneonone',\n",
              " 'oneonone',\n",
              " 'extroverts',\n",
              " 'talksocialized',\n",
              " 'itmight',\n",
              " 'attened',\n",
              " 'whilesome',\n",
              " 'attened',\n",
              " 'becasue',\n",
              " 'counselour',\n",
              " 'becasue',\n",
              " 'conseolur',\n",
              " 'enought',\n",
              " 'conclusioni',\n",
              " 'comfotable',\n",
              " 'dazes',\n",
              " 'handily',\n",
              " 'doter',\n",
              " 'wii',\n",
              " 'anteing',\n",
              " 'belived',\n",
              " 'belived',\n",
              " 'enjoye',\n",
              " 'belived',\n",
              " 'sitution',\n",
              " 'expericing',\n",
              " 'belives',\n",
              " 'disaprove',\n",
              " 'disapointing',\n",
              " 'chocies',\n",
              " 'haning',\n",
              " 'otherside',\n",
              " 'graduted',\n",
              " 'sucessful',\n",
              " 'belived',\n",
              " 'youself',\n",
              " 'depening',\n",
              " 'europea',\n",
              " 'bosina',\n",
              " 'hergionvian',\n",
              " 'catholicorthdox',\n",
              " 'fasion',\n",
              " 'germen',\n",
              " 'differnt',\n",
              " 'belived',\n",
              " 'sucessful',\n",
              " 'belived',\n",
              " 'belived',\n",
              " 'anyting',\n",
              " 'someting',\n",
              " 'postive',\n",
              " 'sucess',\n",
              " 'negitive',\n",
              " 'belived',\n",
              " 'succes',\n",
              " 'postive',\n",
              " '',\n",
              " 'coferencing',\n",
              " 'wounld',\n",
              " 'sudent',\n",
              " 'cannot',\n",
              " 'cannot',\n",
              " 'cannot',\n",
              " 'alllow',\n",
              " '',\n",
              " 'acorting',\n",
              " 'finachely',\n",
              " 'sexesfull',\n",
              " 'fineachely',\n",
              " 'prows',\n",
              " 'egucashon',\n",
              " 'withour',\n",
              " 'succes',\n",
              " 'withour',\n",
              " 'snthusiasm',\n",
              " 'churchills',\n",
              " 'dosnt',\n",
              " 'succesful',\n",
              " 'withour',\n",
              " 'unti',\n",
              " 'withour',\n",
              " 'unti',\n",
              " 'hisher',\n",
              " 'peaple',\n",
              " 'enjoined',\n",
              " 'bestfriend',\n",
              " 'secands',\n",
              " 'remenver',\n",
              " 'yuears',\n",
              " 'deciide',\n",
              " 'thigs',\n",
              " 'worrie',\n",
              " 'seccesful',\n",
              " 'boisboll',\n",
              " 'beisboll',\n",
              " 'churchills',\n",
              " 'successs',\n",
              " 'afil',\n",
              " '',\n",
              " 'expreience',\n",
              " 'alwaysa',\n",
              " 'stufdf',\n",
              " 'whith',\n",
              " 'theyll',\n",
              " 'closly',\n",
              " 'adivse',\n",
              " 'fueture',\n",
              " 'alwaytake',\n",
              " 'happem',\n",
              " 'pice',\n",
              " 'adivse',\n",
              " 'rember',\n",
              " '',\n",
              " 'bordom',\n",
              " 'spead',\n",
              " 'attendince',\n",
              " 'diffent',\n",
              " 'hearig',\n",
              " 'mojor',\n",
              " 'droped',\n",
              " 'stessed',\n",
              " 'happend',\n",
              " 'privaledge',\n",
              " 'angery',\n",
              " 'aultications',\n",
              " 'transperations',\n",
              " 'adukts',\n",
              " 'transpertation',\n",
              " 'forchen',\n",
              " '',\n",
              " '',\n",
              " 'selfreliance',\n",
              " 'valus',\n",
              " 'selfreiance',\n",
              " 'defin',\n",
              " 'seenk',\n",
              " 'stuffclothshoesphone',\n",
              " 'knowshe',\n",
              " 'nikefiaadidasjordankd',\n",
              " 'boughting',\n",
              " 'expanve',\n",
              " 'differnt',\n",
              " 'everysingle',\n",
              " 'becuse',\n",
              " 'teenger',\n",
              " 'expanve',\n",
              " 'exsenve',\n",
              " 'teenger',\n",
              " 'differnt',\n",
              " 'expance',\n",
              " 'becasue',\n",
              " 'airpod',\n",
              " 'airpods',\n",
              " 'chrismas',\n",
              " 'airpods',\n",
              " 'eles',\n",
              " 'airpods',\n",
              " 'alllways',\n",
              " 'vaule',\n",
              " 'becasue',\n",
              " 'founf',\n",
              " 'differnt',\n",
              " 'expernts',\n",
              " 'amsad',\n",
              " '',\n",
              " 'thir',\n",
              " 'changebut',\n",
              " 'vising',\n",
              " 'churchil',\n",
              " 'oponion',\n",
              " 'persuit',\n",
              " 'enthusiam',\n",
              " 'rifht',\n",
              " 'succes',\n",
              " 'controle',\n",
              " 'taht',\n",
              " 'churchills',\n",
              " 'thas',\n",
              " 'feelas',\n",
              " 'perosn',\n",
              " 'comig',\n",
              " 'becasue',\n",
              " 'accition',\n",
              " 'meam',\n",
              " 'examplethat',\n",
              " 'faiure',\n",
              " 'dowm',\n",
              " 'churchiil',\n",
              " 'accion',\n",
              " 'bewteen',\n",
              " 'accion',\n",
              " 'succes',\n",
              " 'enthusiams',\n",
              " 'dowm',\n",
              " 'importat',\n",
              " 'importnat',\n",
              " 'depents',\n",
              " 'stament',\n",
              " 'actuation',\n",
              " 'succes',\n",
              " 'attetion',\n",
              " 'enthusiams',\n",
              " 'youn',\n",
              " 'accion',\n",
              " 'acction',\n",
              " 'wiil',\n",
              " 'bewteen',\n",
              " 'oponion',\n",
              " 'persuit',\n",
              " 'enthusiam',\n",
              " 'rifht',\n",
              " 'depents',\n",
              " 'breain',\n",
              " 'meam',\n",
              " '',\n",
              " 'traying',\n",
              " 'gratest',\n",
              " 'liket',\n",
              " 'belieb',\n",
              " 'beliebe',\n",
              " 'traying',\n",
              " 'selfs',\n",
              " 'traying',\n",
              " 'ypour',\n",
              " '',\n",
              " 'beacuase',\n",
              " 'fougth',\n",
              " 'transfering',\n",
              " 'condifenthard',\n",
              " 'workingresposible',\n",
              " '',\n",
              " 'curfews',\n",
              " 'footstep',\n",
              " 'lucikly',\n",
              " 'curfews',\n",
              " 'cannot',\n",
              " 'curfews',\n",
              " 'molarity',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'threatenting',\n",
              " 'curfews',\n",
              " 'controling',\n",
              " 'curfews',\n",
              " 'estalishment',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'carrried',\n",
              " 'consequense',\n",
              " 'erases',\n",
              " 'unpleasure',\n",
              " 'curfews',\n",
              " 'awakeness',\n",
              " 'curfews',\n",
              " 'unfortuantely',\n",
              " 'curfews',\n",
              " 'teenagerss',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'aggressor',\n",
              " 'chenging',\n",
              " 'experince',\n",
              " 'schol',\n",
              " 'experince',\n",
              " 'souce',\n",
              " 'spaicy',\n",
              " 'holtes',\n",
              " 'salphamore',\n",
              " 'salphamore',\n",
              " 'freshmens',\n",
              " 'acctually',\n",
              " 'vetimans',\n",
              " 'fery',\n",
              " 'weakand',\n",
              " 'boodies',\n",
              " 'absotutly',\n",
              " 'cheper',\n",
              " 'whaen',\n",
              " 'salphamore',\n",
              " 'happend',\n",
              " 'happend',\n",
              " 'htats',\n",
              " 'typers',\n",
              " 'arguement',\n",
              " 'acctualy',\n",
              " 'principale',\n",
              " 'schoolwork',\n",
              " 'yearround',\n",
              " 'daytoday',\n",
              " 'eventhough',\n",
              " 'openmindedly',\n",
              " 'openminded',\n",
              " 'detirriation',\n",
              " 'overworking',\n",
              " 'detirriating',\n",
              " 'councilors',\n",
              " 'yearround',\n",
              " 'openmindedly',\n",
              " 'destress',\n",
              " '',\n",
              " 'aditude',\n",
              " 'beacuase',\n",
              " 'aditude',\n",
              " 'aditude',\n",
              " 'foward',\n",
              " 'adidute',\n",
              " 'pacient',\n",
              " 'aditude',\n",
              " 'benifets',\n",
              " 'aditude',\n",
              " 'benifets',\n",
              " 'aditute',\n",
              " 'referal',\n",
              " 'aditude',\n",
              " 'aditude',\n",
              " 'continusly',\n",
              " 'sheve',\n",
              " 'aditude',\n",
              " 'acomplish',\n",
              " 'aditude',\n",
              " '',\n",
              " '',\n",
              " 'charcter',\n",
              " 'diferent',\n",
              " 'chacter',\n",
              " 'diffrent',\n",
              " 'charcter',\n",
              " 'charateristic',\n",
              " 'charcteristic',\n",
              " 'chacteristic',\n",
              " 'inturduce',\n",
              " 'laguage',\n",
              " 'conterble',\n",
              " 'descibe',\n",
              " 'charcteristic',\n",
              " 'descibe',\n",
              " 'descibed',\n",
              " 'pretents',\n",
              " 'happeaned',\n",
              " 'descibe',\n",
              " 'charcter',\n",
              " 'seing',\n",
              " 'chacteristic',\n",
              " 'chacter',\n",
              " 'charcter',\n",
              " 'atitud',\n",
              " 'charcter',\n",
              " 'speical',\n",
              " 'charteristic',\n",
              " 'charcter',\n",
              " 'charcter',\n",
              " 'diffrents',\n",
              " 'charateristic',\n",
              " 'languge',\n",
              " 'chacteristic',\n",
              " 'charater',\n",
              " 'ofothers',\n",
              " 'wast',\n",
              " '',\n",
              " 'internetwe',\n",
              " 'managmentby',\n",
              " 'itshe',\n",
              " 'exausted',\n",
              " 'cerain',\n",
              " 'caos',\n",
              " 'interupting',\n",
              " 'enery',\n",
              " 'exausting',\n",
              " 'clases',\n",
              " 'expirience',\n",
              " 'activitys',\n",
              " 'activitys',\n",
              " 'activitys',\n",
              " 'activitys',\n",
              " 'activitys',\n",
              " 'princpal',\n",
              " 'activitys',\n",
              " 'themdo',\n",
              " 'insidethats',\n",
              " '',\n",
              " 'payed',\n",
              " 'payed',\n",
              " 'payed',\n",
              " 'disscustions',\n",
              " 'intrest',\n",
              " 'enojy',\n",
              " 'disscustions',\n",
              " 'argree',\n",
              " 'contrarie',\n",
              " 'asand',\n",
              " 'toghter',\n",
              " 'cannot',\n",
              " 'procrastinate',\n",
              " 'mutate',\n",
              " 'fascinate',\n",
              " '',\n",
              " 'peaty',\n",
              " 'mathreading',\n",
              " 'serous',\n",
              " 'gradeand',\n",
              " 'statementsuch',\n",
              " 'timesomething',\n",
              " 'problemand',\n",
              " 'becausemore',\n",
              " 'partisanand',\n",
              " 'momis',\n",
              " 'howeverhaving',\n",
              " 'addishlythe',\n",
              " 'benefice',\n",
              " 'pertipat',\n",
              " 'intrdust',\n",
              " 'disided',\n",
              " 'pertipat',\n",
              " 'howeverpartisan',\n",
              " 'namedidnt',\n",
              " 'becauseher',\n",
              " 'misted',\n",
              " 'concluss',\n",
              " '',\n",
              " 'intrested',\n",
              " 'learm',\n",
              " '',\n",
              " 'wrights',\n",
              " 'resmay',\n",
              " 'resmay',\n",
              " 'expriens',\n",
              " 'intrested',\n",
              " 'searouse',\n",
              " 'recordask',\n",
              " 'badthings',\n",
              " 'conterable',\n",
              " 'conterable',\n",
              " 'wrok',\n",
              " 'hierd',\n",
              " 'hierd',\n",
              " 'resaymay',\n",
              " 'profectionalthey',\n",
              " 'hierd',\n",
              " 'interviwe',\n",
              " 'drees',\n",
              " 'niceso',\n",
              " 'wuold',\n",
              " 'inerviwe',\n",
              " 'hiredand',\n",
              " 'chancese',\n",
              " 'doning',\n",
              " 'interviwe',\n",
              " 'intervewing',\n",
              " 'interveiwing',\n",
              " 'interveiw',\n",
              " 'nervose',\n",
              " 'pratice',\n",
              " 'nervose',\n",
              " 'interveiw',\n",
              " 'wahts',\n",
              " 'expreance',\n",
              " '',\n",
              " 'activies',\n",
              " '',\n",
              " 'defenitly',\n",
              " 'tecaher',\n",
              " 'defenitly',\n",
              " 'indepentdently',\n",
              " 'threes',\n",
              " 'tecahers',\n",
              " 'electonic',\n",
              " 'whil',\n",
              " 'tecaheing',\n",
              " 'abiut',\n",
              " 'reteake',\n",
              " 'responsibilty',\n",
              " '',\n",
              " 'contrition',\n",
              " 'butyour',\n",
              " 'studyhard',\n",
              " 'bacuese',\n",
              " '',\n",
              " 'suppceful',\n",
              " 'beyod',\n",
              " 'importan',\n",
              " 'beyod',\n",
              " 'pisitive',\n",
              " 'caan',\n",
              " 'importan',\n",
              " 'importan',\n",
              " 'beyod',\n",
              " 'importan',\n",
              " 'caan',\n",
              " 'somenthing',\n",
              " 'beyod',\n",
              " 'achive',\n",
              " 'somenthing',\n",
              " 'beyod',\n",
              " '',\n",
              " 'paceyou',\n",
              " 'schoolwork',\n",
              " 'familiesand',\n",
              " 'becasue',\n",
              " 'balacing',\n",
              " 'schoolworking',\n",
              " 'diffcult',\n",
              " 'intrenet',\n",
              " 'recommed',\n",
              " 'differents',\n",
              " '',\n",
              " 'camaras',\n",
              " 'addressnews',\n",
              " 'tittle',\n",
              " 'axemple',\n",
              " 'programthat',\n",
              " 'classwork',\n",
              " 'divice',\n",
              " 'conclution',\n",
              " 'inportant',\n",
              " 'everyoneand',\n",
              " 'eveything',\n",
              " 'benefic',\n",
              " '',\n",
              " 'lincol',\n",
              " 'shyand',\n",
              " 'acommplish',\n",
              " 'accommplish',\n",
              " 'thst',\n",
              " 'familes',\n",
              " 'sussed',\n",
              " 'talentskill',\n",
              " 'omelet',\n",
              " 'barey',\n",
              " 'offter',\n",
              " 'auderstant',\n",
              " 'pawing',\n",
              " '',\n",
              " 'positiveness',\n",
              " 'defiantly',\n",
              " 'metier',\n",
              " 'himher',\n",
              " '',\n",
              " 'intertain',\n",
              " 'attension',\n",
              " 'texting',\n",
              " 'texting',\n",
              " 'texting',\n",
              " 'andor',\n",
              " 'cannot',\n",
              " 'elses',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'dancethey',\n",
              " 'curfews',\n",
              " 'curfews',\n",
              " 'resistence',\n",
              " '',\n",
              " 'dou',\n",
              " 'everyto',\n",
              " 'successf',\n",
              " 'keed',\n",
              " 'estudy',\n",
              " 'fot',\n",
              " 'leaver',\n",
              " 'fotr',\n",
              " 'aproximatein',\n",
              " 'everybodyr',\n",
              " 'persuare',\n",
              " '',\n",
              " 'outiside',\n",
              " 'embarassment',\n",
              " 'payed',\n",
              " 'eduacation',\n",
              " 'benefitting',\n",
              " 'dipoma',\n",
              " '',\n",
              " 'somethingbecause',\n",
              " 'donethen',\n",
              " 'dint',\n",
              " 'fells',\n",
              " '',\n",
              " 'occassions',\n",
              " 'religous',\n",
              " 'maembers',\n",
              " 'restaurante',\n",
              " 'satying',\n",
              " 'makup',\n",
              " 'everone',\n",
              " 'literlly',\n",
              " 'photshoping',\n",
              " 'inpropreate',\n",
              " 'cyberbullying',\n",
              " 'cyberbulling',\n",
              " 'photoshopping',\n",
              " 'wouldbe',\n",
              " 'actully',\n",
              " 'besed',\n",
              " 'responsabilities',\n",
              " 'intul',\n",
              " 'oportunities',\n",
              " 'succecful',\n",
              " 'coment',\n",
              " 'gona',\n",
              " 'sacrifed',\n",
              " 'hoobies',\n",
              " 'becuse',\n",
              " 'deservet',\n",
              " 'succes',\n",
              " 'inpossible',\n",
              " 'remenber',\n",
              " 'bussines',\n",
              " 'succesfull',\n",
              " 'oportunities',\n",
              " '',\n",
              " 'porcess',\n",
              " 'schoolthose',\n",
              " 'opportunites',\n",
              " 'relly',\n",
              " 'discovere',\n",
              " 'clothe',\n",
              " 'responsability',\n",
              " 'responsabilitys',\n",
              " 'herhis',\n",
              " 'hisher',\n",
              " 'adultd',\n",
              " 'intersting',\n",
              " '',\n",
              " 'hte',\n",
              " '',\n",
              " 'becase',\n",
              " 'themself',\n",
              " 'anothers',\n",
              " 'themself',\n",
              " 'stessfull',\n",
              " 'diches',\n",
              " 'loundry',\n",
              " 'ourself',\n",
              " 'stressfull',\n",
              " 'homeworks',\n",
              " 'thet',\n",
              " 'schooil',\n",
              " 'wll',\n",
              " 'afect',\n",
              " 'reponsible',\n",
              " 'homeworks',\n",
              " '',\n",
              " 'vincible',\n",
              " 'axc',\n",
              " 'aligick',\n",
              " 'studentsand',\n",
              " 'causeing',\n",
              " 'peoplestudents',\n",
              " 'pertfecty',\n",
              " 'werid',\n",
              " 'perfeshonal',\n",
              " 'alergys',\n",
              " 'werid',\n",
              " 'peryfically',\n",
              " 'studnets',\n",
              " 'alergys',\n",
              " 'themselfs',\n",
              " 'anyother',\n",
              " 'foucs',\n",
              " 'finsh',\n",
              " 'breakes',\n",
              " 'clases',\n",
              " 'clases',\n",
              " 'libary',\n",
              " 'inconclusion',\n",
              " 'learnig',\n",
              " 'themselfs',\n",
              " '',\n",
              " 'gratuated',\n",
              " 'stuied',\n",
              " 'studing',\n",
              " 'pleople',\n",
              " 'ttwo',\n",
              " 'bettter',\n",
              " 'whenhim',\n",
              " 'oportunity',\n",
              " 'shool',\n",
              " 'studyng',\n",
              " 'meed',\n",
              " 'bacuse',\n",
              " '',\n",
              " 'eccept',\n",
              " 'emersons',\n",
              " 'statment',\n",
              " 'dosent',\n",
              " 'mindlessly',\n",
              " 'nutrisous',\n",
              " 'stonger',\n",
              " 'nutrisous',\n",
              " 'nutrisous',\n",
              " 'nutrisous',\n",
              " 'nutrisous',\n",
              " 'aint',\n",
              " 'wroth',\n",
              " 'inconclousion',\n",
              " '',\n",
              " 'impossbile',\n",
              " 'nieve',\n",
              " 'especailly',\n",
              " 'minuteshoursdaysmonths',\n",
              " 'satistfaction',\n",
              " '',\n",
              " 'cristiano',\n",
              " 'cristiano',\n",
              " 'payed',\n",
              " '',\n",
              " '',\n",
              " 'whlie',\n",
              " 'achive',\n",
              " 'achive',\n",
              " 'achiving',\n",
              " 'yhour',\n",
              " 'cannot',\n",
              " 'beliving',\n",
              " 'achive',\n",
              " 'privilages',\n",
              " 'challage',\n",
              " 'challege',\n",
              " 'planty',\n",
              " 'challage',\n",
              " 'achive',\n",
              " '',\n",
              " 'ecucators',\n",
              " 'vacatios',\n",
              " 'relast',\n",
              " 'exampleif',\n",
              " 'doset',\n",
              " 'countrys',\n",
              " 'chafes',\n",
              " '',\n",
              " 'beggining',\n",
              " 'nessecarly',\n",
              " '',\n",
              " 'relaxs',\n",
              " 'examplemy',\n",
              " '',\n",
              " 'firts',\n",
              " 'peolpe',\n",
              " 'finalcially',\n",
              " 'evething',\n",
              " 'soometimes',\n",
              " 'somenthing',\n",
              " 'whta',\n",
              " 'tey',\n",
              " 'anydebitos',\n",
              " 'peolpe',\n",
              " 'livi',\n",
              " 'econom',\n",
              " 'palce',\n",
              " 'pleople',\n",
              " 'hte',\n",
              " 'choises',\n",
              " 'evething',\n",
              " 'doevething',\n",
              " 'diferent',\n",
              " 'everething',\n",
              " 'alway',\n",
              " 'slipperyand',\n",
              " 'homeand',\n",
              " 'conclusionschools',\n",
              " 'collegesuniversities',\n",
              " 'talkand',\n",
              " 'worldor',\n",
              " 'thingsand',\n",
              " 'moreand',\n",
              " 'collagegraduate',\n",
              " 'upsucceed',\n",
              " 'doingand',\n",
              " 'thingsand',\n",
              " 'someoneand',\n",
              " 'hardand',\n",
              " 'thingsand',\n",
              " 'fotbool',\n",
              " 'basquelbal',\n",
              " ...]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#interesting data vizualization\n",
        "unknown_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "np_idf[0] = .5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "50000\n"
          ]
        }
      ],
      "source": [
        "print(len(np_train_tf_essays[0]))\n",
        "print(len(np_idf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_idf_per_essay = np_train_tf_essays/np_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_idf_per_essay_test = np_test_tf_essays/np_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1955"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tf_idf_per_essay_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SQw9bM2bBGFa"
      },
      "outputs": [],
      "source": [
        "#Implement tf-idf over all essays\n",
        "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "tfIdf = tfIdfVectorizer.fit_transform(np_all_essays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "U4iO1TCaBGFb"
      },
      "outputs": [],
      "source": [
        "#split tf-idf vectors and associated vocab scores into a training and testing \n",
        "#data set\n",
        "embedded_training = []\n",
        "embedded_testing = []\n",
        "score_training = []\n",
        "score_testing = []\n",
        "\n",
        "for index, essay in enumerate(np_all_essays):\n",
        "  tf_idf_per_word = tfIdf[index].toarray()[0]\n",
        "  essay_score = np_all_scores[index]\n",
        "  if index % 2 == 0:\n",
        "    embedded_training.append(tf_idf_per_word)\n",
        "    score_training.append(essay_score)\n",
        "  else:\n",
        "    embedded_testing.append(tf_idf_per_word)\n",
        "    score_testing.append(essay_score)\n",
        "\n",
        "np_embedded_training = np.array(embedded_training)\n",
        "np_embedded_testing = np.array(embedded_testing)\n",
        "np_score_training= np.array(score_training)\n",
        "np_score_testing = np.array(score_testing)\n",
        "\n",
        "#currently unused code\n",
        "\n",
        "#df = pd.DataFrame(tfIdf[index].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "#tf_idf_per_word = np.array([data[1] for data in df.iterrows()])\n",
        "#indices=np.array(tfIdfVectorizer.get_feature_names())\n",
        "#df = df.sort_values('TF-IDF', ascending=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jjPz__hJBGFb"
      },
      "outputs": [],
      "source": [
        "#create word to index and index to word mapping to make looking up values easy\n",
        "indices=np.array(tfIdfVectorizer.get_feature_names_out())\n",
        "ind_to_word = {} \n",
        "word_to_ind = {}\n",
        "for index, word in enumerate(indices):\n",
        "  ind_to_word[index] = word\n",
        "  word_to_ind[word] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbKcnXIYBGFc",
        "outputId": "36c82c80-1ad7-4bbd-f275-a7a627b4513f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\\ndf = df.sort_values(\\'TF-IDF\\', ascending=False)\\nprint (df.head(50))'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "df = df.sort_values('TF-IDF', ascending=False)\n",
        "print (df.head(50))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF_FvdmKBGFc"
      },
      "source": [
        "**IMPORTANT**:\n",
        "To reverse engineer TF-IDF, SKlearn does a few things different.\n",
        "\n",
        "- TF is calculated by # of times word appears in document.\n",
        "- IDF is calculated by log base e ([1 + # documents] / [1 + # documents word appears in])\n",
        "- our IDF = log base e ([1 + count for 'the'] / [count for each word])\n",
        "\n",
        "Once all the words have TF-IDF scores, they are normalized by dividing by the square of all the other vectors\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "EkYV72M-BGFd"
      },
      "outputs": [],
      "source": [
        "#Will be useful later\n",
        "idf_vector_for_every_word = tfIdfVectorizer.idf_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zM_wvFEr6ck"
      },
      "source": [
        "**READ ME for Charlie's work:**\n",
        "\n",
        "*Main takeaway:* Building a model for only TF-IDF encodings is straightforward, but we might want to consider other options.\n",
        "\n",
        "*   TF-IDF encodings are fixed-length (21,363 words) single-dimensional vectors.\n",
        "*   No sequential (word-order) information is stored in them, so neither an RNN (including LSTM) nor CNN make sense for this input type.\n",
        "*   Therefore, it makes sense to just use a simple NN (achieved 0.117 MSE on training set and 0.269 on testing).\n",
        "\n",
        "*This approach might be good enough, and we can try to apply it or implement an RNN (LSTM) for one or two of the other metrics.*\n",
        "\n",
        "If we want to use the RNN approach for vocab, it gets a little complicated. One option is to replace the words in each essay with each TF-IDF value, then run an LSTM on that. This removes any indication of *which* words are being used, but it will give us info on how they are used (i.e. Are lots of basic words being used together, or are rarer words being mixed in?). My conclusion is that it might not be worth this effort because we don't know if it will improve the accuracy substantially.\n",
        "\n",
        "**Extra note: I experimented with changing the scoring method gives very promising results. Using a softmax output layer to predict the score as 0, 0.5, 1, 1.5,..., 4.5, 5 instead of relu gives better accuarcy and MSE during testing, but worse for training. Obviously an overfitting issue in this case, and I think overfitting will be the biggest challenge for everything.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TyT1cKc0BGFd"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "#number of distinct words, and therefore our input layer\n",
        "num_vars = len(indices)\n",
        "\n",
        "#at this point, we have a training set of TF-IDF vectors and a testing set, \n",
        "# as well as the associated scores. next step is to create a NN to connect them\n",
        "#  and create weights\n",
        "\n",
        "#Conv1D(input_shape=(21363), filters=20, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "#    Conv1D(filters=40, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(2000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO6Ux8wiBUUA",
        "outputId": "a95d1546-33ab-4a69-8f39-9f04060b4f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 36s 291ms/step - loss: 0.0560 - accuracy: 0.0010\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 27s 223ms/step - loss: 0.0537 - accuracy: 0.0010\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 27s 223ms/step - loss: 0.0494 - accuracy: 0.0010\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 28s 225ms/step - loss: 0.0462 - accuracy: 0.0010\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 29s 235ms/step - loss: 0.0468 - accuracy: 0.0010\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 27s 221ms/step - loss: 0.0450 - accuracy: 0.0010\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0406 - accuracy: 0.0010\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0363 - accuracy: 5.1125e-04\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0327 - accuracy: 0.0010\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0307 - accuracy: 5.1125e-04\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 28s 231ms/step - loss: 0.0327 - accuracy: 5.1125e-04\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 28s 229ms/step - loss: 0.0316 - accuracy: 0.0010\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 28s 227ms/step - loss: 0.0327 - accuracy: 0.0010\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 28s 224ms/step - loss: 0.0352 - accuracy: 0.0010\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 27s 223ms/step - loss: 0.0266 - accuracy: 0.0010\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 28s 225ms/step - loss: 0.0269 - accuracy: 0.0010\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 28s 224ms/step - loss: 0.0277 - accuracy: 0.0010\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 28s 231ms/step - loss: 0.0281 - accuracy: 0.0010\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 27s 221ms/step - loss: 0.0242 - accuracy: 0.0010\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0259 - accuracy: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ab8645640>"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(np_embedded_training, np_score_training, batch_size=16, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P0R8I_6rMA5",
        "outputId": "10c2579f-ccc5-4aa6-b0f5-aacb06d09122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 6s 89ms/step - loss: 0.2749 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.27491435408592224, 0.0]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(np_embedded_testing, np_score_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test new model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#number of distinct words, and therefore our input layer\n",
        "num_vars = len(vocab_word_to_index)\n",
        "\n",
        "#at this point, we have a training set of TF-IDF vectors and a testing set, \n",
        "# as well as the associated scores. next step is to create a NN to connect them\n",
        "#  and create weights\n",
        "\n",
        "#Conv1D(input_shape=(21363), filters=20, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "#    Conv1D(filters=40, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "model2 = Sequential([\n",
        "    Dense(5000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1956\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1956"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(tf_idf_per_essay))\n",
        "len(np_scores_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 66s 518ms/step - loss: 3.1639 - accuracy: 0.0032\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 63s 509ms/step - loss: 1.5439 - accuracy: 0.0032\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 61s 492ms/step - loss: 1.2034 - accuracy: 0.0032\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 58s 468ms/step - loss: 0.7640 - accuracy: 0.0032\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 60s 488ms/step - loss: 0.5917 - accuracy: 0.0032\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 58s 470ms/step - loss: 0.4306 - accuracy: 0.0032\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 57s 463ms/step - loss: 0.3928 - accuracy: 0.0032\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 57s 462ms/step - loss: 0.3810 - accuracy: 0.0032\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 57s 462ms/step - loss: 0.3565 - accuracy: 0.0032\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 58s 468ms/step - loss: 0.3465 - accuracy: 0.0032\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 58s 471ms/step - loss: 0.3204 - accuracy: 0.0032\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 58s 474ms/step - loss: 0.3103 - accuracy: 0.0032\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 57s 462ms/step - loss: 0.3046 - accuracy: 0.0032\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 58s 468ms/step - loss: 0.2839 - accuracy: 0.0032\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 57s 464ms/step - loss: 0.2876 - accuracy: 0.0032\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 56s 456ms/step - loss: 0.2733 - accuracy: 0.0032\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 57s 460ms/step - loss: 0.2725 - accuracy: 0.0032\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 57s 460ms/step - loss: 0.2755 - accuracy: 0.0032\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 57s 461ms/step - loss: 0.2677 - accuracy: 0.0032\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 56s 458ms/step - loss: 0.2498 - accuracy: 0.0032\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15c7229a0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.fit(tf_idf_per_essay, np_scores_train, batch_size=16, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 6s 93ms/step - loss: 0.3255 - accuracy: 0.0015\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.32551485300064087, 0.0015345268184319139]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.evaluate(tf_idf_per_essay_test, np_scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.save(\"common_words_tf-idf_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KUjjLyFBGFd"
      },
      "source": [
        "- use LSTMS on word embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "#number of distinct words, and therefore our input layer\n",
        "num_vars = len(vocab_word_to_index)\n",
        "\n",
        "#at this point, we have a training set of TF-IDF vectors and a testing set, \n",
        "# as well as the associated scores. next step is to create a NN to connect them\n",
        "#  and create weights\n",
        "\n",
        "#Conv1D(input_shape=(21363), filters=20, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "#    Conv1D(filters=40, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "model3 = Sequential([\n",
        "    Dense(2000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 24.        ,   3.54098918,  48.55919941, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 32.        ,  14.16395673,  10.40554273, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 24.        ,  31.86890264,   6.93702849, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       ...,\n",
              "       [ 12.        ,   0.        ,  24.2795997 , ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 16.        ,  10.62296755,  13.87405697, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [108.        ,  10.62296755,  97.11839881, ...,   0.        ,\n",
              "          0.        ,   0.        ]])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf_per_essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 6s 255ms/step - loss: 3.0916 - accuracy: 0.0028\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 5s 250ms/step - loss: 1.9745 - accuracy: 0.0032\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 5s 252ms/step - loss: 1.4483 - accuracy: 0.0032\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 5s 256ms/step - loss: 1.0356 - accuracy: 0.0032\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 5s 258ms/step - loss: 0.8050 - accuracy: 0.0032\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 5s 262ms/step - loss: 0.6391 - accuracy: 0.0032\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 5s 265ms/step - loss: 0.5174 - accuracy: 0.0032\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.4410 - accuracy: 0.0032\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.3999 - accuracy: 0.0032\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.3472 - accuracy: 0.0032\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.3573 - accuracy: 0.0032\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.3255 - accuracy: 0.0032\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.3035 - accuracy: 0.0032\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 5s 271ms/step - loss: 0.2775 - accuracy: 0.0032\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2782 - accuracy: 0.0032\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 5s 274ms/step - loss: 0.2787 - accuracy: 0.0032\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2724 - accuracy: 0.0032\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2531 - accuracy: 0.0032\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 5s 273ms/step - loss: 0.2465 - accuracy: 0.0032\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2416 - accuracy: 0.0032\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2391 - accuracy: 0.0032\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2301 - accuracy: 0.0032\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.2270 - accuracy: 0.0032\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2195 - accuracy: 0.0032\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 0.2225 - accuracy: 0.0032\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.2197 - accuracy: 0.0032\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2159 - accuracy: 0.0032\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.2267 - accuracy: 0.0032\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.2162 - accuracy: 0.0032\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 5s 265ms/step - loss: 0.2142 - accuracy: 0.0032\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 6s 278ms/step - loss: 0.2110 - accuracy: 0.0032\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2117 - accuracy: 0.0032\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 5s 268ms/step - loss: 0.2181 - accuracy: 0.0032\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2100 - accuracy: 0.0032\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.2015 - accuracy: 0.0032\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.2086 - accuracy: 0.0032\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.2064 - accuracy: 0.0032\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.1960 - accuracy: 0.0032\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 5s 267ms/step - loss: 0.1947 - accuracy: 0.0032\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.1953 - accuracy: 0.0032\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 5s 262ms/step - loss: 0.1996 - accuracy: 0.0032\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1951 - accuracy: 0.0032\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 5s 266ms/step - loss: 0.1901 - accuracy: 0.0032\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1884 - accuracy: 0.0032\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1895 - accuracy: 0.0032\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1892 - accuracy: 0.0032\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1842 - accuracy: 0.0032\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 5s 263ms/step - loss: 0.1830 - accuracy: 0.0032\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1786 - accuracy: 0.0032\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 0.1846 - accuracy: 0.0032\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x15c3b8ee0>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.fit(tf_idf_per_essay, np_scores_train, batch_size=100, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 3s 49ms/step - loss: 0.4011 - accuracy: 0.0015\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4010857343673706, 0.0015345268184319139]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.evaluate(tf_idf_per_essay_test, np_scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3.save(\"common_words_tf-idf_new_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqtDVhuHBGFT"
      },
      "source": [
        "# TF-IDF of essays \n",
        "This jupyter notebook takes in a pool of essays, and constructs the TF-IDF score and trains a neural network to take in a TF-IDF matrix and compute a vocabulary score based on this information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "I-XDiuc8BGFV",
        "outputId": "a01392d7-7310-4b51-97d7-2eaeacbc4c3f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nthis is the way to get kaggle files I think\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "'''\n",
        "this is the way to get kaggle files I think\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "'''\n",
        "\n",
        "#hello\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZmO3V5XHBGFX"
      },
      "outputs": [],
      "source": [
        "#read in csv file\n",
        "full_train_data = pd.read_csv('./data/train.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KdrgQ7GGBGFY"
      },
      "outputs": [],
      "source": [
        "#Reads in 333,000 most common words in English language, not currently used later in code but might be useful...\n",
        "all_vocab_words = pd.read_csv('./data/unigram_freq.csv', index_col=0)\n",
        "all_vocabs = np.array([data[0] for data in all_vocab_words.iterrows()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qf40-kLDBGFY"
      },
      "outputs": [],
      "source": [
        "#parse over every row, store essay in the essays_train/test lists, and store the 6 scores in the scores_train/test list\n",
        "essays_train = []\n",
        "scores_train = []\n",
        "essays_test = []\n",
        "scores_test = []\n",
        "all_essays = []\n",
        "all_vocabs_words = []\n",
        "\n",
        "#Technique for splitting essays into training and testing data and results\n",
        "'''\n",
        "for ind, data in enumerate(full_train_data.iterrows()):\n",
        "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
        "    essays_train.append(text) if ind % 2 == 0 else essays_test.append(text)\n",
        "    scores_train.append([cohes, syntax, vocab, phrase, gram, convs]) if ind % 2 == 0 else scores_test.append(\n",
        "        [cohes, syntax, vocab, phrase, gram, convs]) '''\n",
        "\n",
        "for ind, data in enumerate(full_train_data.iterrows()):\n",
        "    text, cohes, syntax, vocab, phrase, gram, convs = data[1]\n",
        "    all_essays.append(text)\n",
        "    all_vocabs_words.append(vocab) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FzmhtBA2BGFY"
      },
      "outputs": [],
      "source": [
        "#make into np arrays, currently not used\n",
        "np_essays_train = np.array(essays_train)\n",
        "np_essays_test = np.array(essays_test)\n",
        "np_scores_train = np.array(scores_train)\n",
        "np_scores_test = np.array(scores_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LdjAvuUNBGFa"
      },
      "outputs": [],
      "source": [
        "#make tfIdf for all essays in bank\n",
        "#np_all_essays = np.concatenate((essays_train, essays_test), axis = 0)\n",
        "np_all_essays = np.array(all_essays)\n",
        "np_all_scores = np.array(all_vocabs_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SQw9bM2bBGFa"
      },
      "outputs": [],
      "source": [
        "#Implement tf-idf over all essays\n",
        "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "tfIdf = tfIdfVectorizer.fit_transform(np_all_essays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "U4iO1TCaBGFb"
      },
      "outputs": [],
      "source": [
        "#split tf-idf vectors and associated vocab scores into a training and testing \n",
        "#data set\n",
        "embedded_training = []\n",
        "embedded_testing = []\n",
        "score_training = []\n",
        "score_testing = []\n",
        "\n",
        "for index, essay in enumerate(np_all_essays):\n",
        "  tf_idf_per_word = tfIdf[index].toarray()[0]\n",
        "  essay_score = np_all_scores[index]\n",
        "  if index % 2 == 0:\n",
        "    embedded_training.append(tf_idf_per_word)\n",
        "    score_training.append(essay_score)\n",
        "  else:\n",
        "    embedded_testing.append(tf_idf_per_word)\n",
        "    score_testing.append(essay_score)\n",
        "\n",
        "np_embedded_training = np.array(embedded_training)\n",
        "np_embedded_testing = np.array(embedded_testing)\n",
        "np_score_training= np.array(score_training)\n",
        "np_score_testing = np.array(score_testing)\n",
        "\n",
        "#currently unused code\n",
        "\n",
        "#df = pd.DataFrame(tfIdf[index].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "#tf_idf_per_word = np.array([data[1] for data in df.iterrows()])\n",
        "#indices=np.array(tfIdfVectorizer.get_feature_names())\n",
        "#df = df.sort_values('TF-IDF', ascending=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jjPz__hJBGFb"
      },
      "outputs": [],
      "source": [
        "#create word to index and index to word mapping to make looking up values easy\n",
        "indices=np.array(tfIdfVectorizer.get_feature_names_out())\n",
        "ind_to_word = {} \n",
        "word_to_ind = {}\n",
        "for index, word in enumerate(indices):\n",
        "  ind_to_word[index] = word\n",
        "  word_to_ind[word] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbKcnXIYBGFc",
        "outputId": "36c82c80-1ad7-4bbd-f275-a7a627b4513f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\\ndf = df.sort_values(\\'TF-IDF\\', ascending=False)\\nprint (df.head(50))'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "df = df.sort_values('TF-IDF', ascending=False)\n",
        "print (df.head(50))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF_FvdmKBGFc"
      },
      "source": [
        "**IMPORTANT**:\n",
        "To reverse engineer TF-IDF, SKlearn does a few things different.\n",
        "\n",
        "- TF is calculated by # of times word appears in document.\n",
        "- IDF is calculated by log base e ([1 + # documents] / [1 + # documents word appears in])\n",
        "\n",
        "Once all the words have TF-IDF scores, they are normalized by dividing by the square of all the other vectors\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "EkYV72M-BGFd"
      },
      "outputs": [],
      "source": [
        "#Will be useful later\n",
        "idf_vector_for_every_word = tfIdfVectorizer.idf_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zM_wvFEr6ck"
      },
      "source": [
        "**READ ME for Charlie's work:**\n",
        "\n",
        "*Main takeaway:* Building a model for only TF-IDF encodings is straightforward, but we might want to consider other options.\n",
        "\n",
        "*   TF-IDF encodings are fixed-length (21,363 words) single-dimensional vectors.\n",
        "*   No sequential (word-order) information is stored in them, so neither an RNN (including LSTM) nor CNN make sense for this input type.\n",
        "*   Therefore, it makes sense to just use a simple NN (achieved 0.117 MSE on training set and 0.269 on testing).\n",
        "\n",
        "*This approach might be good enough, and we can try to apply it or implement an RNN (LSTM) for one or two of the other metrics.*\n",
        "\n",
        "If we want to use the RNN approach for vocab, it gets a little complicated. One option is to replace the words in each essay with each TF-IDF value, then run an LSTM on that. This removes any indication of *which* words are being used, but it will give us info on how they are used (i.e. Are lots of basic words being used together, or are rarer words being mixed in?). My conclusion is that it might not be worth this effort because we don't know if it will improve the accuracy substantially.\n",
        "\n",
        "**Extra note: I experimented with changing the scoring method gives very promising results. Using a softmax output layer to predict the score as 0, 0.5, 1, 1.5,..., 4.5, 5 instead of relu gives better accuarcy and MSE during testing, but worse for training. Obviously an overfitting issue in this case, and I think overfitting will be the biggest challenge for everything.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TyT1cKc0BGFd"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "#number of distinct words, and therefore our input layer\n",
        "num_vars = len(indices)\n",
        "\n",
        "#at this point, we have a training set of TF-IDF vectors and a testing set, \n",
        "# as well as the associated scores. next step is to create a NN to connect them\n",
        "#  and create weights\n",
        "\n",
        "#Conv1D(input_shape=(21363), filters=20, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "#    Conv1D(filters=40, kernel_size=5, strides=1, padding='valid', activation='relu'),\n",
        "#    MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(2000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO6Ux8wiBUUA",
        "outputId": "a95d1546-33ab-4a69-8f39-9f04060b4f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 36s 291ms/step - loss: 0.0560 - accuracy: 0.0010\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 27s 223ms/step - loss: 0.0537 - accuracy: 0.0010\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 27s 223ms/step - loss: 0.0494 - accuracy: 0.0010\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 28s 225ms/step - loss: 0.0462 - accuracy: 0.0010\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 29s 235ms/step - loss: 0.0468 - accuracy: 0.0010\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 27s 221ms/step - loss: 0.0450 - accuracy: 0.0010\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0406 - accuracy: 0.0010\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0363 - accuracy: 5.1125e-04\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0327 - accuracy: 0.0010\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0307 - accuracy: 5.1125e-04\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 28s 231ms/step - loss: 0.0327 - accuracy: 5.1125e-04\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 28s 229ms/step - loss: 0.0316 - accuracy: 0.0010\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 28s 227ms/step - loss: 0.0327 - accuracy: 0.0010\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 28s 224ms/step - loss: 0.0352 - accuracy: 0.0010\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 27s 223ms/step - loss: 0.0266 - accuracy: 0.0010\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 28s 225ms/step - loss: 0.0269 - accuracy: 0.0010\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 28s 224ms/step - loss: 0.0277 - accuracy: 0.0010\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 28s 231ms/step - loss: 0.0281 - accuracy: 0.0010\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 27s 221ms/step - loss: 0.0242 - accuracy: 0.0010\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 27s 222ms/step - loss: 0.0259 - accuracy: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ab8645640>"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(np_embedded_training, np_score_training, batch_size=16, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P0R8I_6rMA5",
        "outputId": "10c2579f-ccc5-4aa6-b0f5-aacb06d09122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 6s 89ms/step - loss: 0.2749 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.27491435408592224, 0.0]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(np_embedded_testing, np_score_testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KUjjLyFBGFd"
      },
      "source": [
        "- use LSTMS on word embeddings\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
